
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../about">ABOUT</a> &nbsp;<a class="title" href="../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">July 27, 2022</p>

<p><title>Hyperparameter optimization: KerasTuner &amp; TensorBoard</title></p>

<h1 id="hyperparameter-optimization">Hyperparameter optimization</h1>

<p>Finding the optimal model architecture and training configuration is a tedious and time-consuming task.
The manual process of repeatedly tuning a model's hyperparameters and training configuration often leads to sub-optimal model performance.</p>

<p><em>Hyperparameters</em> are values that are used to control the model's learning process during training.
Their values determine the model's performance - specifically, the model's ability to correctly map the input data to the desired labels or targets.
The more optimal the hyperparameters, the better the model's performance.</p>

<p>In deep learning models, the most common hyperparameters are the number of hidden layers, the number of neurons in each layer, and the activation function used in each layer.</p>

<p><details>
<summary>Common hyperparameters</summary></p>

<ul>
<li>Train-validation-test split ratio</li>
<li>Optimizer algorithm (e.g., gradient descent, stochastic gradient descent, or Adam optimizer)</li>
<li>Optimizer's learning-rate</li>
<li>Convolutional layer's kernel or filter size</li>
<li>Activation function in a neural network layer (e.g. Sigmoid, ReLU, Tanh)</li>
<li>Number of hidden layers</li>
<li>Number of activation units in each layer</li>
<li>Dropout rate</li>
<li>Pooling size</li>
<li>Batch size</li>
<li>Number of iterations (epochs) during training</li>
<li>Number of clusters in a clustering task</li>
</ul>

<p></details></p>

<p>We can use <a href="https://keras.io/keras_tuner/">KerasTuner</a> to automate the process of hyperparameter optimization.
<a href="https://www.tensorflow.org/tensorboard/">TensorBoard</a> visualizer can be used alongside KerasTuner to visualize the optimization progress.</p>

<p>This article will cover the basics of hyperparameter optimization in deep learning projects using KerasTuner and TensorBoard.
The examples will be based on my own <a href="../toonvision/classification">ToonVision</a> computer vision project.</p>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#hyperparameter-optimization">Hyperparameter optimization</a>
<ul>
<li><a href="#project-description">Project description</a>
<ul>
<li><a href="#model-architecture">Model architecture</a></li>
<li><a href="#hyperparameters">Hyperparameters</a></li>
</ul></li>
<li><a href="#kerastuner">KerasTuner</a>
<ul>
<li><a href="#define-the-hyperparameter-search-space">Define the hyperparameter search space</a>
<ul>
<li><a href="#search-space-considerations">Search space considerations</a></li>
</ul></li>
<li><a href="#create-a-tuner-object">Create a tuner object</a></li>
<li><a href="#launch-the-tuning-process">Launch the tuning process</a>
<ul>
<li><a href="#tuning-process-search-times">Tuning process search times</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<p></details></p>

<hr />

<h2 id="project-description">Project description</h2>

<p>The ToonVision project is a multiclass classification model for classifying <a href="https://toontownrewritten.fandom.com/wiki/Cogs">Cogs</a> in ToonTown Online.
There are four unique Cog types - also called <a href="https://toontownrewritten.fandom.com/wiki/Corporate_ladder">corporate ladders</a> or suits.
Our goal is to train a model that can classify Cogs into the four unique suits, as seen in the image below.</p>

<figure class="center">
    <img src="img/unique_cogs.png" style="width:100%;"/>
    <figcaption>Unique Cog types: Bossbot, Lawbot, Cashbot, Sellbot</figcaption>
</figure>

<h3 id="model-architecture">Model architecture</h3>

<p>We'll create a model from scratch and use my <a href="../toonvision/classification/#the-toonvision-dataset">ToonVision dataset</a> to train and evaluate the model.</p>

<p>The model will be a convolutional neural network (CNN).
It will have two "blocks", each of which contains a single convolutional layer, two max pooling layers, and a dropout layer.
The final layer will be a fully-connected layer (Dense) with four output nodes, one for each of the four Cog types.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">make_multiclass_model</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Rescaling</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Block 1: Conv2d -&gt; MaxPool2D -&gt; MaxPool2D -&gt; Dropout</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Block 2: Conv2D -&gt; MaxPool2D -&gt; MaxPool2D -&gt; Dropout</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(),</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">()],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

<h3 id="hyperparameters">Hyperparameters</h3>

<p>The model's hyperparameters were chosen by intuition and experimentation.
However, I believe that we can find better hyperparameters by tuning the model's hyperparameters using KerasTuner.</p>

<p>We'll focus on tuning the following hyperparameters with KerasTuner:</p>

<ul>
<li><code>filters</code>: The number of convolutional filters in each convolutional layer.</li>
<li><code>kernel_size</code>: The size of the convolutional kernel.</li>
<li><code>pool_size</code>: The size of the max pooling layers.</li>
<li><code>dropout_rate</code>: The probability of dropping a neuron.</li>
</ul>

<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<p>Additional hyperparameter tunings could include the number of layers (convolutional/pooling/dropout), optimizer algorithm, and learning rate, but I will not cover these here.</p>

<p>Before we start tuning the hyperparameters, let's discuss what KerasTuner does and how it helps ML engineers.</p>

<hr />

<h2 id="kerastuner">KerasTuner</h2>

<p>KerasTuner is a general-purpose hyperparameter tuning library.
The library is well integrated with Keras, allowing for hyperparameter tuning with minimal code changes.
It truly is a powerful, yet simple, library.</p>

<p>We can begin tuning with three easy steps:</p>

<ol>
<li>Define the desired hyperparameter search space</li>
<li>Create a KerasTuner tuner object of type <code>Hyperband</code>, <code>BayesianOptimization</code>, or <code>RandomSearch</code></li>
<li>Launch the tuning process</li>
</ol>

<p>Pretty simple, right?
Let's take a look at how we can implement the above steps.</p>

<h3 id="define-the-hyperparameter-search-space">Define the hyperparameter search space</h3>

<p>Defining a search space is as simple as replacing the layers' hyperparameter values with KerasTuner's search space methods: <code>hp.Int</code>, <code>hp.Float</code>, <code>hp.Choice</code>, etc.
More details about the KerasTuner search space methods can be found <a href="https://keras.io/api/keras_tuner/hyperparameters/">here</a>.</p>

<p>For instance, the follow code block defines a search space for the number of convolutional filters in some convolutional layer.
When launched, the tuner searches for the most optimal filter count by varying the number of filters in the layer from 4 to 16 and training the model.</p>

<div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">Int</span><span class="p">(</span><span class="s2">&quot;conv_1_filters&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">])</span>
</code></pre></div>

<p>What was once a tedious, manual task is now simple and powerful process for ML engineers.</p>

<p>The following code block is our model-building function with defined search spaces.
Recall that we're searching for the most optimal filter count, kernel size, pooling sizes, and dropout rate.</p>

<p>Note the use of <code>hp.Int</code>, <code>hp.Float</code>, and <code>hp.Choice</code> methods in each layer.
Each of these methods defines a search space for the corresponding hyperparameter.
Integers and floats are used for discrete search spaces (minimum and maximum values with steps), while choices are used for categorical search spaces.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">model_builder</span><span class="p">(</span><span class="n">hp</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="c1"># Input and augmentation layers</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Rescaling</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">255</span><span class="p">),</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal&quot;</span><span class="p">),</span>

            <span class="c1"># Block 1: Conv2D -&gt; MaxPool2D -&gt; MaxPool2D -&gt; Dropout</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">Int</span><span class="p">(</span><span class="s2">&quot;conv_1_filters&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">Choice</span><span class="p">(</span><span class="s2">&quot;conv_1_kernel_size&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span>
                <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span>
                <span class="n">pool_size</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">Int</span><span class="p">(</span><span class="s2">&quot;pool_1_size&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="c1"># Min value == 1 will void the second pooling layer</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span>
                <span class="n">pool_size</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">Int</span><span class="p">(</span><span class="s2">&quot;pool_2_size&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span>
                <span class="n">rate</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">Float</span><span class="p">(</span><span class="s2">&quot;dropout_1_rate&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="o">...</span>  <span class="c1"># Repeat for Block 2 (omitted for brevity)</span>

            <span class="c1"># Output layer</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(),</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">()],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

<h4 id="search-space-considerations">Search space considerations</h4>

<p>Selecting the correct methods and values for the search space is critical to the success of the tuning process.
We do not want such a large search space that the tuner takes too much time and resources.
However, we also do not want such a small search space that the tuner does not find any optimal hyperparameters.</p>

<p>Rather, we must consider meaningful values for each hyperparameter.
This is where intuition, experimentation, and domain expertise comes in to help us define the search space.</p>

<p>For my model, I knew that the number of convolutional filters should remain low (4 to 16).
This choice was made in part because I wanted to avoid overfitting to the validation data during training.
However, I also knew from experience that the more filters I have, the lower my model's generalization performance.</p>

<p>Furthermore, I selected two MaxPooling2D layers for each block because I knew the main differentiation between classes is the Cog's suit color.
My intuition says that more pooling is better, but I'm putting it to the test by defining a search space that also evaluates only a single MaxPooling2D layer.
This is how domain expertise - knowing your data's characteristics - helps us define meaningful search spaces.</p>

<h3 id="create-a-tuner-object">Create a tuner object</h3>

<p>KerasTuner contains multiple tuners: <code>RandomSearch</code>, <code>BayesianOptimization</code>, and <code>Hyperband</code>.
Each has their own unique tuning algorithm, but all of them share the same search space defined above.
Here are the three tuners along with their respective algorithms:</p>

<ul>
<li><code>kerastuner.tuners.randomsearch.RandomSearch</code>: An inefficient, random search algorithm.</li>
<li><code>kerastuner.tuners.bayesian.BayesianOptimization</code>: A Bayesian optimization algorithm that follows a probabilistic search approach by taking previous results into account.</li>
<li><code>kerastuner.tuners.hyperband.Hyperband</code>: An optimized variant of the <code>RandomSearch</code> algorithm in terms of time and resource usage.</li>
</ul>

<p>More details above each tuner can be found in <a href="https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms">this article</a>.
Additionally, refer to the <a href="https://keras.io/api/keras_tuner/tuners/">KerasTuner documentation</a> for API details.</p>

<p>My preferred tuning method is to first perform a <code>RandomSearch</code> with a large number of trials (100+).
Each trial samples a random set of hyperparameter values from the search space.
The goal is to find the best hyperparameter values that minimizes (or maximizes) the objective - in our case, the goal is minimizing the validation loss.</p>

<div class="codehilite"><pre><span></span><code><span class="n">tuner</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span>
    <span class="n">hypermodel</span><span class="o">=</span><span class="n">model_builder</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
    <span class="n">max_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">executions_per_trial</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Increase to reduce variance of the results</span>
    <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;models&quot;</span><span class="p">,</span>
    <span class="n">project_name</span><span class="o">=</span><span class="s2">&quot;tuned_multiclass_randomsearch&quot;</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p><code>RandomSearch</code> is the least efficient algorithm, but it provides useful insight into the general whereabouts of optimal hyperparameter values.
These insights can be used to further reduce the search space for more effective tuning.</p>

<p>Following the random search, I'll review the highest performing parameters in TensorBoard, tighten my search space, and then launch a more efficient <code>Hyperband</code> or <code>BayesianOptimization</code> search.
Let's launch a <code>RandomSearch</code> and review the results.</p>

<h3 id="launch-the-tuning-process">Launch the tuning process</h3>

<p>The tuning process uses identical arguments as the <code>keras.Model.fit</code> method.
Refer to the code block below to see how the <code>RandomSearch</code> is launched.</p>

<p>We will utilize the <code>tf.keras.callbacks.TensorBoard</code> callback to monitor the tuning process' progress.
This callback will save the logs of all trials to the <code>./tb_logs/randomsearch/</code> directory.
We can then use TensorBoard to visualize the results of all trials during/after the tuning process.</p>

<div class="codehilite"><pre><span></span><code><span class="n">tuner</span><span class="o">.</span><span class="n">search</span><span class="p">(</span>
    <span class="n">train_images</span><span class="p">,</span>
    <span class="n">train_labels</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">),</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="s2">&quot;./tb_logs/randomsearch/&quot;</span><span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>
</code></pre></div>

<p>Details about the TensorBoard callback API can be found <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard">here</a>.
A Keras guide for visualizing the tuning process can be found <a href="https://keras.io/guides/keras_tuner/visualize_tuning/">here</a>.</p>

<h4 id="tuning-process-search-times">Tuning process search times</h4>

<p>On a GPU, 100 trials of <code>RandomSearch</code> with the search space above takes roughly 45 minutes.
The search would take even longer if done on a CPU.</p>

<p>We can reduce the search time by constraining the search space, reducing the number of trials, decreasing the number of epochs, and/or reducing the executions per trial.
Alternatively, we could pick a more efficient algorithm, such as <code>Hyperband</code> or <code>BayesianOptimization</code>.</p>

<p>Search times are also dependent on the size of the model - filters in the Conv2D layers or pooling sizes in MaxPooling2D layers.
That's why it's important to define the search space with meaningful values; if the values are needlessly large, the search will be inefficient with regards to time and computation.</p>

</body>
</html>
