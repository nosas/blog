
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../about">ABOUT</a> &nbsp;<a class="title" href="../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">August 05, 2022</p>

<p><title>Performance Measures for Classification Problems</title></p>

<h1 id="classification-performance-measures">Classification Performance Measures</h1>

<p>This article will explain the most common performance measures for classifications problems.
These measures apply to both binary and multi-class classification problems.</p>

<p>We will explain model performance metrics such as confusion matrix, accuracy, precision, recall, F1-score, and ROC curve.
The code in this article utilizes python3.7, tensorflow, and keras.</p>

<ul>
<li><a href="#classification-performance-measures">Classification Performance Measures</a>
<ul>
<li><a href="#why-are-performance-measures-important">Why are performance measures important?</a></li>
<li><a href="#confusion-matrix">Confusion Matrix</a>
<ul>
<li><a href="#tp-tn-fp-and-fn">TP, TN, FP, and FN</a></li>
<li><a href="#generate-confusion-matrix-for-tensorflow-model">Generate confusion matrix for TensorFlow model</a></li>
</ul></li>
<li><a href="#accuracy">Accuracy</a></li>
<li><a href="#precision">Precision</a></li>
<li><a href="#accuracy-vs-precision">Accuracy vs Precision</a></li>
<li><a href="#recall">Recall</a></li>
<li><a href="#f1-score">F1-score</a></li>
<li><a href="#roc-curve">ROC Curve</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>

<hr />

<h2 id="why-are-performance-measures-important">Why are performance measures important?</h2>

<p>During training, we monitor how well the model performs on the training data using the loss and accuracy metrics.
While these metrics are useful for monitoring the progress of the model, they are not very useful for evaluating the <em>performance</em>, or <em>quality</em>, of the model.</p>

<p>For example, imagine we've trained 100 models for the same classification problem, each with a different set of hyperparameters.
How do we know which model is the best?
Do we pick the model with the lowest loss or highest accuracy model?</p>

<p>We could pick the model with the lowest loss, or highest accuracy, but that does not guarantee that the model is the best.
Alternatively, we could pick the model with the least amount of wrong predictions on the test data.
But does that mean that the model is the best?</p>

<p>The loss and accuracy metrics give us a rough idea of the model's performance on the training data, but no indication of the model's general performance.
In order to gain a better understanding of the model's performance, we must use more specific metrics.
The metrics shown later in the article are designed to evaluate the true performance of our classification models.</p>

<hr />

<h2 id="confusion-matrix">Confusion Matrix</h2>

<p>A confusion matrix is a technique for visualizing a classification model's performance.
As the name suggests, a confusion matrix is a 2-dimensional table.
The confusion matrix is a core part of evaluating a classification model.</p>

<p>The table below shows the confusion matrix for a binary classification problem.
The rows represent the true labels and the columns represent the predicted labels.</p>

<p><font style="color:red">TODO: Insert binary confusion matrix</font></p>

<p>We can expand the confusion matrix to include multi-class classification problems.
For example, the table below shows the confusion matrix for a multi-class classification problem.</p>

<p><font style="color:red">TODO: Insert multiclass confusion matrix</font></p>

<p>From the confusion matrix, we can calculate the accuracy of the model - the number of correct predictions divided by the total number of predictions.
Furthermore, we can determine the number of true positives, true negatives, false positives, and false negatives.
We'll shorten the names to TP, TN, FP, and FN, respectively.
Using TP, TN, FP, and FN, we can calculate the precision, recall, and F1-score of the model.</p>

<h3 id="tp-tn-fp-and-fn">TP, TN, FP, and FN</h3>

<p>Later in this article, we'll use a confusion matrix to derive the accuracy, precision, recall, and F1-score of our classification models.</p>

<h3 id="generate-confusion-matrix-for-tensorflow-model">Generate confusion matrix for TensorFlow model</h3>

<p><font style="color:red">TODO: Insert code to generate confusion matrix for TensorFlow model</font></p>

<hr />

<h2 id="accuracy">Accuracy</h2>

<p>Accuracy is a metric that measures the percentage of correct predictions across all classes.
In other words, accuracy is how close the model comes to the correct result.</p>

<p>For example, imagine the goal is to shoot an arrow and hit an apple.
If we shoot and hit 10 arrows, we would be accurate or have high <em>accuracy</em>.
Now imagine a cluster of arrows around an apple - the arrows were close to hitting the apple, but had no guarantee of hitting the apple.
This remains a case of high accuracy, but with low <em>precision</em>.
We'll talk about <em>precision</em> in the next section.</p>

<p>It's calculated by dividing the number of correct predictions by the total number of predictions.
We calculate accuracy as follows: (TP + TN) / (TP + TN + FP + FN), where TP, TN, FP, and FN are the true positives, true negatives, false positives, and false negatives, respectively.</p>

<p><font style="color:red">TODO: Insert code snippet to calculate accuracy</font></p>

<p>During training, we can use Keras' built-in accuracy metrics.
For binary classification models, we user the <code>binary_accuracy</code> metric.
For multi-class classification models, we use the <code>[sparse_]categorical_accuracy</code> metric.</p>

<p><font style="color:red">TODO: Insert code snippet using Keras' built-tin accuracy metrics</font></p>

<!-- ? Given wrong_preds and target_labels, can we use the metric methods by themselves? -->

<hr />

<h2 id="precision">Precision</h2>

<hr />

<h2 id="accuracy-vs-precision">Accuracy vs Precision</h2>

<hr />

<h2 id="recall">Recall</h2>

<hr />

<h2 id="f1-score">F1-score</h2>

<hr />

<h2 id="roc-curve">ROC Curve</h2>

<hr />

<h2 id="conclusion">Conclusion</h2>

</body>
</html>
