
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../about">ABOUT</a> &nbsp;<a class="title" href="../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">August 02, 2022</p>

<p><title>Performance Measures for Classification Problems</title></p>

<h1 id="classification-performance-measures">Classification Performance Measures</h1>

<p>This article will explain the most common performance measures for classifications problems.
These measures apply to both binary and multi-class classification problems.</p>

<p>We will explain model performance metrics such as confusion matrix, accuracy, precision, recall, F1-score, and ROC curve.
The code blocks in this article will utilize python3.7, tensorflow, and keras.</p>

<ul>
<li><a href="#classification-performance-measures">Classification Performance Measures</a>
<ul>
<li><a href="#why-are-performance-measures-important">Why are performance measures important?</a></li>
<li><a href="#confusion-matrix">Confusion Matrix</a></li>
<li><a href="#accuracy">Accuracy</a></li>
<li><a href="#precision">Precision</a></li>
<li><a href="#recall">Recall</a></li>
<li><a href="#f1-score">F1-score</a></li>
<li><a href="#roc-curve">ROC Curve</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>

<hr />

<h2 id="why-are-performance-measures-important">Why are performance measures important?</h2>

<p>During training, we monitor how well the model performs on the training data using the loss and accuracy metrics.
While these metrics are useful for monitoring the progress of the model, they are not very useful for evaluating the <em>performance</em>, or <em>quality</em>, of the model.</p>

<p>For example, imagine we've trained 100 models for the same classification problem, each with a different set of hyperparameters.
How do we know which model is the best?
Do we pick the model with the lowest loss or highest accuracy model?</p>

<p>We could pick the model with the lowest loss, or highest accuracy, but that does not guarantee that the model is the best.
Alternatively, we could pick the model with the least amount of wrong predictions on the test data.
But does that mean that the model is the best?</p>

<p>The loss and accuracy metrics give us a rough idea of the model's performance on the training data, but no indication of the model's general performance.
In order to gain a better understanding of the model's performance, we must use more specific metrics.
The metrics shown later in the article are designed to evaluate the true performance of our classification models.</p>

<hr />

<h2 id="confusion-matrix">Confusion Matrix</h2>

<hr />

<h2 id="accuracy">Accuracy</h2>

<hr />

<h2 id="precision">Precision</h2>

<hr />

<h2 id="recall">Recall</h2>

<hr />

<h2 id="f1-score">F1-score</h2>

<hr />

<h2 id="roc-curve">ROC Curve</h2>

<hr />

<h2 id="conclusion">Conclusion</h2>

</body>
</html>
