
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../../about">ABOUT</a> &nbsp;<a class="title" href="../../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">May 23, 2022</p>

<p><title>Deep Learning with Python: Chapter 3 - Introduction to Keras and TensorFlow</title></p>

<h1 id="deep-learning-with-python-omit-in-toc-">Deep Learning with Python  <!-- omit in toc --></h1>

<p>This article is part 3/13 (?) of a series of articles named <em>Deep Learning with Python</em>.</p>

<p>In this series, I will read through the second edition of <em>Deep Learning with Python</em> by Fran√ßois Chollet.
Articles in this series will sequentially review key concepts, examples, and interesting facts from each chapter of the book.</p>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#chapter-3-introduction-to-keras-and-tensorflow">Chapter 3: Introduction to Keras and TensorFlow</a>
<ul>
<li><a href="#whats-tensorflow">What's TensorFlow?</a>
<ul>
<li><a href="#tensorflow-vs-numpy">TensorFlow vs. NumPy</a></li>
<li><a href="#tensorflow-ecosystem">TensorFlow ecosystem</a></li>
</ul></li>
<li><a href="#whats-keras">What's Keras?</a>
<ul>
<li><a href="#keras-and-tensorflow-a-brief-history">Keras and TensorFlow: A brief history</a></li>
</ul></li>
<li><a href="#setting-up-a-deep-learning-workspace">Setting up a deep learning workspace</a>
<ul>
<li><a href="#physical-machine-with-nvidia-gpu">Physical machine with NVIDIA GPU</a></li>
<li><a href="#cloud-gpu-instances">Cloud GPU instances</a></li>
<li><a href="#google-colab">Google Colab</a></li>
</ul></li>
<li><a href="#first-steps-with-tensorflow">First steps with TensorFlow</a>
<ul>
<li><a href="#constant-tensors-and-variables">Constant tensors and variables</a></li>
<li><a href="#a-second-look-at-the-gradient-tape-api">A second look at the Gradient Tape API</a></li>
<li><a href="#computing-second-order-gradients">Computing second-order gradients</a></li>
</ul></li>
<li><a href="#linear-classifier-example-in-pure-tensorflow">Linear classifier example in pure TensorFlow</a>
<ul>
<li><a href="#what-is-linear-classification">What is linear classification?</a></li>
<li><a href="#generating-synthetic-data">Generating synthetic data</a></li>
<li><a href="#creating-the-linear-classifier">Creating the linear classifier</a></li>
<li><a href="#training-the-linear-classifier">Training the linear classifier</a></li>
<li><a href="#plotting-the-loss">Plotting the loss</a>
</details></li>
</ul></li>
</ul></li>
</ul>

<hr />

<h1 id="chapter-3-introduction-to-keras-and-tensorflow">Chapter 3: Introduction to Keras and TensorFlow</h1>

<p>This chapter covers...</p>

<ul>
<li>A closer look at TensorFlow, Keras, and their relationship</li>
<li>Setting up a deep learning workspace</li>
<li>Review of how deep learning concepts learned in previous chapters translate to Keras and TensorFlow</li>
</ul>

<p>This chapter gives us everything required to get started with deep learning.
By the end of this chapter, we'll be ready to move on to practical, real-world applications of deep learning.</p>

<hr />

<h2 id="whats-tensorflow">What's TensorFlow?</h2>

<p>TensorFlow is a free and open-source machine learning framework for Python.
It was primarily developed by Google.
Similar to NumPy, it is a general-purpose and efficient numerical library used by engineers to manipulate mathematical expressions using numerical tensors.</p>

<h3 id="tensorflow-vs-numpy">TensorFlow vs. NumPy</h3>

<p>TensorFlow far surpasses NumPy in the following ways:</p>

<ul>
<li>Automatically computes the gradient of any differentiable expressions (as seen in Ch2 with <code>GradientTape</code>)</li>
<li>Runs not only on CPUs, but also on GPUs and TPUs (highly-parallel hardware accelerators)</li>
<li>Computations defined in TensorFlow can be easily distributed across multiple devices (CPUs, GPUs, and TPUs) and machines</li>
<li>TensorFlow programs can be exported and easily deployed to other runtimes, such as C++, JavaScript (for browsers), or TensorFlow lite (for mobile devices or embedded systems)</li>
</ul>

<h3 id="tensorflow-ecosystem">TensorFlow ecosystem</h3>

<p>TensorFlow is much more than a single Python library.
Rather, it's a platform home to a vast ecosystem of components.</p>

<p>Components of the ecosystem include:</p>

<ul>
<li>TF-Agents for reinforcement learning</li>
<li>TF-Hub (repository) for pre-trained deep neural networks</li>
<li>TensorFlow Serving for production deployment</li>
<li>TFX for distributed training and ML workflow managements</li>
</ul>

<p>Together, these components cover a wide-range of use cases: from cutting-edge research to large-scale production, or just a simple image classification application to see if a dog or a cat is in a picture.</p>

<p>Scientists from Oak Ridge National Lab have used TensorFlow to train an extreme weather forecasting model on the 27,000 GPUs within the IBM Summit supercomputer.
Google, on the other hand, has used TensorFlow to develop deep learning applications such as the chess-playing and Go-playing agent AlphaZero.</p>

<hr />

<h2 id="whats-keras">What's Keras?</h2>

<p>Keras is a high-level deep learning API built on top of TensorFlow.
It provides a convenient and flexible API for building and training deep learning models.</p>

<p><font style="color:red">TODO: Insert image of Keras, TF, hardware hierarchical diagram</font></p>

<p>Keras is known for providing a clean, simple, and efficient API that prioritizes the developer experience.
It's an API for human beings, not machines, and follows best practices for reducing cognitive load.
The API provides consistent and simple workflows, minimizes the number of actions required for common use cases, and outputs clear and actionable feedback upon user error.</p>

<p>Much like Python, Keras' large and diverse user base enables a well-documented and wide range of workflows for utilizing the API.
Keras does not force one to follow a single "true" way of building and training models.
Rather, it allows the user to build and train models corresponding to their own preference, and to explore the possibilities of each approach.</p>

<h3 id="keras-and-tensorflow-a-brief-history">Keras and TensorFlow: A brief history</h3>

<p>Keras was designed originally in March 2015 to be used with Theano, a tensor-manipulation library developed by Montreal Institute for Learning Algorithms (MILA).
Theano pioneered the idea of using static computation graphs for automatic differentiation and compiling code to both CPU and GPU support.</p>

<p>Following the release of TensorFlow 1.0 in November 2015, Keras was refactored to support multiple backend architectures: starting with Theano and Tensorflow in late 2015, and adding support for CNTK and MXNet in 2017.</p>

<p>Keras became well known as the user-friendly way to develop TensorFlow applications.
By late 2017, a majority of TensorFlow users were using Keras.
In 2018, the TensorFlow leadership picked Keras and TensorFlow's official high-level API.
As a result, as of September 2019, the Keras API is the official API for TensorFlow 2.0.</p>

<p>Enough of the history, let's learn how to set up a deep learning workspace.</p>

<hr />

<h2 id="setting-up-a-deep-learning-workspace">Setting up a deep learning workspace</h2>

<p>There are a handful of ways to set up a deep learning workspace:</p>

<ul>
<li>Buy and install a physical machine with an NVIDIA GPU</li>
<li>Use GPU instances on AWS, Google Cloud, or cheaper alternatives such as Jarvis Labs</li>
<li>Use the free GPU runtime from Google Colab, a hosted Jupyter notebook service that executes code on GPUs and even TPUs</li>
</ul>

<p>Each approach has its own advantages and disadvantages in terms of flexibility, cost, and ease of use.
I'll briefly discuss the advantages and disadvantages of each approach below, but I will not discuss setup at all.</p>

<h3 id="physical-machine-with-nvidia-gpu">Physical machine with NVIDIA GPU</h3>

<p>Buying a machine with a GPU is not the easiest way to get started with DL, as it requires manual setup and it's also the most expensive upfront.
The upfront cost is amplified by the current (as of May 2022) chip shortage and GPU scalpers.
This method involves installing proper drivers, sorting out version conflicts, and then configuring the libraries to use the GPU instead of the CPU.</p>

<p>Most users already have NVIDIA GPUs installed in their computers.
Given the large user base of TensorFlow, there are many tutorials for setting up NVIDIA GPUs for deep learning, so this is not a bad option for tech-savvy people.</p>

<p>The alternative to buying a GPU is the use of embedded devices built specifically for efficient and highly-parallelized math operations, such as <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/">NVIDIA's Jetson</a> or <a href="https://coral.ai/products/">Google's Coral</a>.</p>

<h3 id="cloud-gpu-instances">Cloud GPU instances</h3>

<p>Using GPU instances is cheaper in the short-term because you pay as you go (per hour basis), but it's not sustainable in the long run if you're a heavy user of deep learning.
The benefit of using GPU instances is that it requires minimal setup as most instances have Python, TensorFlow, and Keras pre-installed - it's mostly plug-and-play and easy to use.
Moreover, the GPU instance can easily be upgraded, torn down, cloned, and re-installed.
Lastly, students and enterprise employees often get discounts - or free usage - for AWS and Google Cloud.</p>

<p>I personally use Jarvis Labs and AWS for my deep learning needs because I have discounts for both services.
There aren't many differences between cloud instance providers other than the availability of high-powered GPUs.</p>

<h3 id="google-colab">Google Colab</h3>

<p>The last approach - using free GPUs from Google Colab - is the simplest way to get started with deep learning.
It's recommended for those who are not familiar with the hardware and software, and for those who are new to deep learning.
Francois himself recommends executing code examples found in the book using Google Colab as it requires the least amount of setup.
The drawback of Colab is that the free GPU is time-limited and shared by users - meaning that the execution may be slower.</p>

<hr />

<h2 id="first-steps-with-tensorflow">First steps with TensorFlow</h2>

<p>Training a neural network revolves around low-level tensor manipulations and high-level deep learning concepts.
TensorFlow takes care of the tensor manipulation through the use of:</p>

<ul>
<li><em>Tensors</em>, including special tensors that store the network's state (<em>variables</em>)</li>
<li><em>Tensor operations</em> such as addition, <code>relu</code>, <code>matmul</code>, etc.
<ul>
<li>The previous article details <a href="https://fars.io/deep_learning_python/ch2/#tensor-operations">tensor operations</a></li>
</ul></li>
<li><em>Backpropagation</em>, a way to compute gradients of mathematical operations (using TensorFlow's <code>GradientTape</code>)
<ul>
<li>The previous article discusses <a href="https://fars.io/deep_learning_python/ch2/#backpropagation">backpropagation</a> and <a href="https://fars.io/deep_learning_python/ch2/#tensorflows-gradient-tape">TensorFlow's GradientTape</a></li>
</ul></li>
</ul>

<p>Let's take a deeper dive into how all of the concepts above translate to TensorFlow.</p>

<h3 id="constant-tensors-and-variables">Constant tensors and variables</h3>

<p>To do anything in TensorFlow, we need to create tensors.
Let's look at code examples for creating tensors with all ones, zeros, or random values:</p>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Equivalent to np.ones((2, 2))</span>
<span class="n">t_ones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># Equivalent to np.zeros((2, 1))</span>
<span class="n">t_zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># Equivalent to np.random.normal(size=(3, 1), loc=0., scale=1)</span>
<span class="n">t_random_normal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="c1"># Equivalent to np.random.uniform(size=(1, 3), low=0., high=1.)</span>
<span class="n">t_random_uniform</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">minval</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">maxvval</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</code></pre></div>

<p>What do the outputs of each tensor look like?</p>

<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">t_ones</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">zeros</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">0.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">t_random_normal</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="o">-</span><span class="mf">0.8276905</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.2264915</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.1399505</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">t_random_uniform</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.141</span> <span class="mf">0.824</span> <span class="mf">0.912</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div>

<p>A significant difference between NumPy arrays and TensorFlow tensors is that tensors are not assignable: they're <em>constant</em>.
For instance, in NumPy, we can assign a value to a tensor, as seen in the code block below.
Whereas, in TensorFlow, we are greeted with an error: <code>TypeError: 'Tensor' object does not support item assignment</code>.</p>

<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t_ones</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="n">File</span> <span class="s2">&quot;&lt;stdin&gt;&quot;</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">TypeError</span><span class="p">:</span> <span class="s1">&#39;Tensor&#39;</span> <span class="nb">object</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">support</span> <span class="n">item</span> <span class="n">assignment</span>
</code></pre></div>

<p>To train a model, however, it's important to be able to change the values of the tensors - update the weights of the model.
This is where the TensorFlow's <em>variable</em> (<code>tf.Variable</code>) comes in to play:</p>

<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.644994</span> <span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.47064</span>  <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.6413262</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
</code></pre></div>

<p>The state of a variable - the entirety of or a subset of coefficients - can be modified via its <code>assign</code> method:</p>

<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">9.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
</code></pre></div>

<p>Similarly, the <code>assign_add()</code> and <code>assign_sub()</code> are tensor-efficient equivalents of <code>+=</code> and <code>-=</code>, respectively.</p>

<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">10.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">2.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">2.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">9.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
</code></pre></div>

<h3 id="a-second-look-at-the-gradient-tape-api">A second look at the Gradient Tape API</h3>

<p>TensorFlow's ability to retrieve gradients of any expression with respect to any of its inputs is what makes the TensorFlow library so powerful.
All we have to do is open a <code>GradientTape</code> context, manipulate the input tensors, and retrieve the gradients with respect to the inputs.</p>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mf">3.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="nb">input</span>

<span class="c1"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</code></pre></div>

<p>The gradient tape is most commonly used to retrieve the gradients of the model's loss with respect to its weights: <code>gradient = tape.gradient(loss, weights)</code>.
We discussed and demonstrated this functionality in the <a href="https://fars.io/deep_learning_python/ch2/#tensorflows-gradient-tape">previous article</a>.</p>

<p>So far, we've only looked at the simplest case of <code>GradientTapes</code> - where the input tensors in <code>tape.gradient()</code> were TensorFlow variables.
It's actually possible for the input tensors to be any arbitrary tensor, not just variables, by calling <code>tape.watch(arbitrary_tensor)</code> within the <code>GradientTape</code> context.</p>

<div class="codehilite"><pre><span></span><code><span class="n">arbitrary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">arbitrary_tensor</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">arbitrary_tensor</span> <span class="o">*</span> <span class="n">arbitrary_tensor</span>

<span class="c1"># tf.Tensor(4.0, shape=(), dtype=float32)</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">arbitrary_tensor</span><span class="p">)</span>
</code></pre></div>

<p>By default, only <em>trainable variables</em> are tracked because computing the gradient of a loss with regard to a trainable variable is the most common use case.
Furthermore, it would be too expensive to preemptively store the information required to compute the gradient of anything with respect to anything.
In an effort avoid wasting resources, only the trainable variables are tracked unless otherwise explicitly specified.</p>

<h3 id="computing-second-order-gradients">Computing second-order gradients</h3>

<p>The gradient tape is a powerful utility capable of computing <em>second-order gradients</em> - or, the gradient of a gradient.</p>

<p>For instance, the gradient of the position of an object with respect to time is the speed of the object.
The second-order gradient of the object speed is its acceleration.</p>

<div class="codehilite"><pre><span></span><code><span class="n">time</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">outer_tape</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">inner_tape</span><span class="p">:</span>
        <span class="n">position</span> <span class="o">=</span> <span class="mf">4.9</span> <span class="o">*</span> <span class="n">time</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">speed</span> <span class="o">=</span> <span class="n">inner_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">time</span><span class="p">)</span>

<span class="c1"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.8&gt;</span>
<span class="n">acceleration</span> <span class="o">=</span> <span class="n">outer_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">speed</span><span class="p">,</span> <span class="n">time</span><span class="p">)</span>
</code></pre></div>

<hr />

<h2 id="linear-classifier-example-in-pure-tensorflow">Linear classifier example in pure TensorFlow</h2>

<p>We now know about tensors, variables, tensor operations, and gradient computation.
That's enough to build any machine learning model based on gradient descent.
Let's put our knowledge to the test and build an end-to-end linear classification model purely in TensorFlow.</p>

<p>We're going to implement a linear classifier that predicts whether a given input belongs to class A or class B.
But first, we need to understand what linear classification is.</p>

<h3 id="what-is-linear-classification">What is linear classification?</h3>

<p>In linear classification problems, the model is trying to find a linear combination of the input features that best predicts the target variable.
Simply put, the model is trying to classify input data into 2+ categories (classes) by drawing a line through the the data.
The line is best fit to separate the data into two classes.</p>

<p><font style="color:red">TODO: Insert image of a linear classification plot with a line separating the classes</font></p>

<p>This is the basic idea behind linear classification.
Now let's generate some data and train a linear classifier.
All of the code related to this linear classifier can be found on my <a href="https://github.com/nosas/blog/blob/main/deep_learning_python/ch3/code/linear_classifier.py">GitHub</a> as an interactive python file.
I recommend using VSCode to utilize the interactive python code blocks - similar to Jupyter Notebooks.</p>

<h3 id="generating-synthetic-data">Generating synthetic data</h3>

<p>We need some nicely linear data to train our linear classifier.
To keep it simple, we'll create two classes of points in a 2D plane and call them class A and class B.
To keep it more simple, we won't explain all the math behind the data generation - just understand that both classes should be clearly separated and roughly distributed like a cloud.
We'll just use the following formula to generate the data:</p>

<div class="codehilite"><pre><span></span><code><span class="n">num_samples_per_class</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">class_a_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span>
    <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">cov</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
    <span class="n">size</span><span class="o">=</span><span class="n">num_samples_per_class</span><span class="p">)</span>
<span class="n">class_b_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span>
    <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">cov</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
    <span class="n">size</span><span class="o">=</span><span class="n">num_samples_per_class</span><span class="p">)</span>
</code></pre></div>

<p>The figure below shows the linearly-separable data from classes A and B.
See the following code block to see how we plot the data.</p>

<figure class="center">
    <img src="img/linear_classifier_data.png" style="width:100%;background:white;"/>
    <figcaption>Two classes of synthetic and random points in the 2D plane</figcaption>
</figure>

<p>Both samples are arrays of shape <code>(500, 2)</code> - meaning there are 500 rows of 2-dimensional data (x and y coordinate points).
Let's stack both class samples into a single array with shape <code>(1000, 2)</code>.
Stacking the samples into single array will allow for easier processing later on, such as plotting the data.</p>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># The first 500 samples are from class A, the next 500 samples are from class B</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">class_a_samples</span><span class="p">,</span> <span class="n">class_b_samples</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># The first 500 labels are 0 (class A), and the next 500 are 1 (class B)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_samples_per_class</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_samples_per_class</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">class_a</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:</span><span class="n">num_samples_per_class</span><span class="p">]</span>
<span class="n">class_b</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">num_samples_per_class</span><span class="p">:]</span>

<span class="c1"># %% Plot the two classes</span>
<span class="c1"># Class A is represented by green dots, and class B is represented by blue dots,</span>
<span class="c1"># plt.scatter(inputs[:, 0], inputs[:, 1], c=labels[:, 0], s=100)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">class_a</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">class_a</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.50</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class A&quot;</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">class_b</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">class_b</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.50</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class B&quot;</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;../img/linear_classifier_data.png&quot;</span><span class="p">,</span> <span class="n">transparent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<h3 id="creating-the-linear-classifier">Creating the linear classifier</h3>

<p>A linear classifier is an <em>affine transformation</em> of the input data (<code>prediction = dot(W, x) + b</code>), trained to minimize the square of the difference (mean squared error, or MSE) between the prediction and the target label.
I have not explained affine transformations - or any geometric interpretations of tensor operations - in my articles, but Francois Chollet greatly details geometric transformations in Chapter 2 of his book.
In short, an affine transformation is the combination of a linear transform (dot product) and a translation (vector addition).</p>

<p>Now that we understand the basic math behind linear classification, let's create the model's variables.</p>

<div class="codehilite"><pre><span></span><code><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>   <span class="c1"># input is a 2D vector</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># output is a scalar, class A &lt; 0.5 &lt; class B</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,)))</span>
</code></pre></div>

<p>Our forward pass function is the affine transformation discussed above.
Our loss function is the mean squared error (MSE) between the prediction and the target label.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">square_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Calculate the loss per sample, results in tensor of shape (len(targets), 1)</span>
    <span class="n">per_sample_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="c1"># Average the per-sample loss and return a single scalar loss value</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">per_sample_loss</span><span class="p">)</span>
</code></pre></div>

<p>Next, we have to train the model.</p>

<h3 id="training-the-linear-classifier">Training the linear classifier</h3>

<p>Let's create the training step, where the model's weights and biases are updated based on the loss.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">square_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="c1"># Calculate the gradients of the loss with respect to the variables</span>
    <span class="n">grad_loss_wrt_W</span><span class="p">,</span> <span class="n">grad_loss_wrt_b</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
    <span class="c1"># Update the variables using the gradients and the learning rate</span>
    <span class="n">W</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_loss_wrt_W</span><span class="p">)</span>
    <span class="n">b</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_loss_wrt_b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>

<p>Finally, let's create the training loop.
For simplicity, we'll do <em>batch training</em> instead of <em>mini-batch training</em>.
Batch training means the model trains on all the data at once instead of iteratively over small batches of the data.</p>

<p>Batch training has its pros and cons: each training step will take much longer to run, since we'll compute the forward pass and gradient calculation for the entire dataset (1000 samples in our example).
On the other hand, because the model is training on the entire dataset, each gradient update will be much more effective at reducing the loss since it learns information from all training samples.</p>

<div class="codehilite"><pre><span></span><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># Save the loss scores so we can plot them later</span>
<span class="n">loss_all</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Save all predictions so we can calculate and plot the accuracy later</span>
<span class="n">predictions_all</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">step_loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">step_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">loss_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step_loss</span><span class="p">)</span>
    <span class="n">predictions_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
</code></pre></div>

<p><details>
    <summary>Full output of loss scores</summary></p>

<pre><code>Step 0: Loss = 1.6673
Step 1: Loss = 0.3011
Step 2: Loss = 0.1401
Step 3: Loss = 0.1145
Step 4: Loss = 0.1048
Step 5: Loss = 0.0975
Step 6: Loss = 0.0910
Step 7: Loss = 0.0851
Step 8: Loss = 0.0797
Step 9: Loss = 0.0747
Step 10: Loss = 0.0702
Step 11: Loss = 0.0661
Step 12: Loss = 0.0624
Step 13: Loss = 0.0590
Step 14: Loss = 0.0559
Step 15: Loss = 0.0530
Step 16: Loss = 0.0505
Step 17: Loss = 0.0481
Step 18: Loss = 0.0459
Step 19: Loss = 0.0440
Step 20: Loss = 0.0422
Step 21: Loss = 0.0405
Step 22: Loss = 0.0391
Step 23: Loss = 0.0377
Step 24: Loss = 0.0365
Step 25: Loss = 0.0353
Step 26: Loss = 0.0343
Step 27: Loss = 0.0333
Step 28: Loss = 0.0325
Step 29: Loss = 0.0317
Step 30: Loss = 0.0310
Step 31: Loss = 0.0303
Step 32: Loss = 0.0297
Step 33: Loss = 0.0292
Step 34: Loss = 0.0287
Step 35: Loss = 0.0283
Step 36: Loss = 0.0278
Step 37: Loss = 0.0275
Step 38: Loss = 0.0271
Step 39: Loss = 0.0268
Step 40: Loss = 0.0265
Step 41: Loss = 0.0263
Step 42: Loss = 0.0260
Step 43: Loss = 0.0258
Step 44: Loss = 0.0256
Step 45: Loss = 0.0254
Step 46: Loss = 0.0253
Step 47: Loss = 0.0251
Step 48: Loss = 0.0250
Step 49: Loss = 0.0249
</code></pre>

<p></details></p>

<p>After 50 epochs, the loss score stabilizes around 0.025.
Let's plot the loss scores to see how the loss score changes after each training step.</p>

<h3 id="plotting-the-loss">Plotting the loss</h3>

<div class="codehilite"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_all</span><span class="p">[:])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</code></pre></div>

<p>It's difficult to see the rate of decrease in the loss score due to the rapid convergence of the loss score.
The initial loss score was initially at 1.6673 on step 0 and dropped to 0.3011 on step 1.
We can improve the plot by excluding the initial loss score.
Refer to the table below where we first plot twice: with all loss scores and all but the initial loss score.</p>

<div class="codehilite"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_all</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</code></pre></div>

<table style="width:100%;">
    <tr>
        <td style="width:50%;">
            <img src="img/loss_all.png" style="background:white; width:100%;">
        </td>
        <td style="width:50%;">
            <img src="img/loss_exclude_initial.png" style="background:white; width:100%;">
        </td>
    </tr>
    <tr >
        <td>
            <span style="text-align:center; display: block; margin-bottom: 2ch;margin-top: 0.5ch;">
                <small>
                    <i>Loss scores of all training steps<i>
                </small>
            </span>
        </td>
        <td>
            <span style="text-align:center; display: block; margin-bottom: 2ch;margin-top: 0.5ch;">
                <small>
                    <i>All loss scores, excluding the initial loss score<i>
                </small>
            </span>
        </td>
    </tr>
</table>

</body>
</html>
