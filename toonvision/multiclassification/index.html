
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../../about">ABOUT</a> &nbsp;<a class="title" href="../../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">July 31, 2022</p>

<p><title>ToonVision: Multiclass Classification</title></p>

<h1 id="toonvision-multiclass-classification">ToonVision - Multiclass Classification</h1>

<p>This article is the second in a series on <strong>ToonVision</strong>.
The <a href="https://fars.io/toonvision/classification/">first article</a> covered the basics of classification and binary classification of Toons and Cogs.
This article covers multiclass classification of Cogs: Cog suits (4 unique suits) and Cog entities (32 unique Cogs).</p>

<p>After reading this article, we'll have a better understanding of how to</p>

<ul>
<li>deal with a model overfitting to a small, imbalanced dataset</li>
<li>utilize image augmentation and dropout to improve the model's generalization capability</li>
<li>compare different models, optimizers, and hyperparameters</li>
<li>interpret and visualize what the model is learning</li>
</ul>

<p>The following article will cover image segmentation of ToonTown's streets, roads, Cogs, and Cog buildings.
Afterwards, we'll implement real-time object detection and, if possible, image segmentation.
For now, let's focus on multiclass classification.</p>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#toonvision---multiclass-classification">ToonVision - Multiclass Classification</a>
<ul>
<li><a href="#toonvision">ToonVision</a></li>
<li><a href="#classification">Classification</a>
<ul>
<li><a href="#multiclass-classification">Multiclass classification</a></li>
</ul></li>
<li><a href="#the-toonvision-dataset">The ToonVision dataset</a>
<ul>
<li><a href="#dataset-considerations">Dataset considerations</a></li>
<li><a href="#why-does-the-street-matter">Why does the street matter?</a></li>
<li><a href="#dataset-balance">Dataset balance</a>
<ul>
<li><a href="#why-does-the-brrrgh-have-the-most-bossbot-samples">Why does The Brrrgh have the most Bossbot samples?</a></li>
<li><a href="#why-are-there-so-many-lawbot-and-sellbot-samples-in-daisys-garden">Why are there so many Lawbot and Sellbot samples in Daisy's Garden?</a></li>
</ul></li>
<li><a href="#creating-the-dataset-objects">Creating the dataset objects</a>
<ul>
<li><a href="#splitting-the-images-into-train-validate-and-test">Splitting the images into train, validate, and test</a></li>
</ul></li>
</ul></li>
<li><a href="#compiling-the-model">Compiling the model</a>
<ul>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#optimizer">Optimizer</a></li>
<li><a href="#metrics">Metrics</a></li>
<li><a href="#callbacks">Callbacks</a></li>
<li><a href="#defining-the-model">Defining the model</a></li>
</ul></li>
<li><a href="#training-the-baseline-model">Training the baseline model</a>
<ul>
<li><a href="#baseline-loss-and-accuracy-plots">Baseline loss and accuracy plots</a></li>
<li><a href="#baseline-wrong-predictions">Baseline wrong predictions</a></li>
</ul></li>
<li><a href="#training-the-optimized-model">Training the optimized model</a>
<ul>
<li><a href="#keras-tuner">Keras Tuner</a></li>
<li><a href="#preventing-overfitting">Preventing overfitting</a></li>
<li><a href="#wrong-predictions">Wrong predictions</a></li>
<li><a href="#baseline-comparison-training">Baseline comparison: Training</a></li>
<li><a href="#baseline-comparison-evaluation">Baseline comparison: Evaluation</a></li>
</ul></li>
<li><a href="#model-interpretation-and-visualization">Model interpretation and visualization</a>
<ul>
<li><a href="#intermediate-convnet-outputs-intermediate-activations">Intermediate convnet outputs (intermediate activations)</a></li>
<li><a href="#convnet-filters">Convnet filters</a></li>
<li><a href="#class-activation-heatmaps">Class activation heatmaps</a>
</details></li>
</ul></li>
</ul></li>
</ul>

<h2 id="toonvision">ToonVision</h2>

<p>ToonVision is my computer vision project for teaching a machine how to see in <a href="https://en.wikipedia.org/wiki/Toontown_Online">ToonTown Online</a> - an MMORPG created by Disney in 2003.
The ultimate goal is to teach a machine (nicknamed <strong>OmniToon</strong>) how to play ToonTown and create a self-sustaining ecosystem where the bots progress through the game together.</p>

<hr />

<h2 id="classification">Classification</h2>

<p>As discussed in the <a href="https://fars.io/toonvision/classification/#classification">previous article</a>, image classification is the process of assigning a label to an input image.
For instance, given a dog-vs-cat classification model and an image of a Pomeranian, the model will predict that the image is a dog.</p>

<p>There are a few variants of the image classification problem: binary, multiclass, multi-label, and so on.
We'll focus on <a href="https://fars.io/toonvision/classification/#multiclass-classification">multiclass classification</a> in this article.</p>

<h3 id="multiclass-classification">Multiclass classification</h3>

<p>Multiclass classification is a problem in which the model predicts which class an input image belongs.
For instance, the model could predict that an animal belongs to the class of dogs, cats, rabbits, horses, or any other animal.</p>

<p>We're building a model to predict which Cog suit a Cog belongs to - a 4-class classification problem.
We'll push the model further by also predicting which Cog entity a Cog belongs to - a 32-class classification problem.
Both multiclass classification problems require us to improve the ToonVision dataset.
The current dataset is imbalanced and does not contain samples of all the Cog suits and entities.
Let's look at how we can improve the dataset in the next section.</p>

<hr />

<h2 id="the-toonvision-dataset">The ToonVision dataset</h2>

<h3 id="dataset-considerations">Dataset considerations</h3>

<p>We'll tweak the existing dataset considerations to focus on balancing the dataset's Cog entity samples.
The current Cog dataset contains two glaring problems:</p>

<ol>
<li>The samples do not account for <em>where</em> - on <em>which street</em> - the Cog is located.</li>
<li>There are no samples of the two highest-ranked Cogs</li>
</ol>

<p>To address problem #1, the ToonVision dataset now requires a balanced number of samples from each of the 6 uniquely designed streets: The Brrrgh, Daisy's Garden, Donald's Dreamland, Donald's Dock, Minnie's Melodyland, and ToonTown Central.
Given that there are 6 streets, the sample images per Cog will increase from 20 to 30; we need 5 images of a Cog from each street.</p>

<p>We'll leverage Cog invasions to ensure we meet the new dataset requirement because some Cogs are not present in certain streets <em>unless</em> there's an ongoing invasion.
Cog invasions will solve both problems moving forward.</p>

<h3 id="why-does-the-street-matter">Why does the street matter?</h3>

<p>Each street in ToonTown has a unique design - colors, houses, floors, obstacles - resulting in unique backgrounds for our extracted objects.
In this case, our extracted objects are Cogs.
If we have a diverse dataset of Cogs with different backgrounds, we can teach our model to better recognize Cog features rather than background features.
Therefore, it's important to take screenshots of Cogs from each street so our model can generalize Cog features across all streets.</p>

<h3 id="dataset-balance">Dataset balance</h3>

<p>The Cog vs Toon dataset is imbalanced with a majority of the dataset belonging to the Cog class.
However, the Cog dataset is mostly balanced.
Our goal is ~30 samples per Cog entity with at least 5 samples per street.
We have achieved the 30 samples per Cog entity requirement, but we're not meeting the 5 samples per street requirement.</p>

<figure class="center" style="width:95%">
    <img src="img/dataset_streets.png" style="width:100%;"/>
    <figcaption>Dataset balance per street</figcaption>
</figure>

<p>There image above shows two notable imbalances.
In the Suits per street graph, we can see:</p>

<ol>
<li>Daisy's Garden (DG) has the most Lawbot and Sellbot samples.</li>
<li>The Brrrgh (BR) has the most Bossbot samples.</li>
</ol>

<h4 id="why-does-the-brrrgh-have-the-most-bossbot-samples">Why does The Brrrgh have the most Bossbot samples?</h4>

<p>The street I frequented to acquire samples in The Brrrgh was <a href="https://toontown.fandom.com/wiki/Walrus_Way">Walrus Way</a>, where the Cog presence is split [90%, 10%] between Bossbots and Lawbots, respectively.
It's no surprise that the majority of the samples from the Brrrgh street are Bossbot samples.</p>

<h4 id="why-are-there-so-many-lawbot-and-sellbot-samples-in-daisys-garden">Why are there so many Lawbot and Sellbot samples in Daisy's Garden?</h4>

<p>The Lawbot imbalance is because the street I visited was majority Lawbot and I took too many screenshots of Bottom Feeders.
The Sellbot imbalance, on the other hand, is due to the Sellbot HQ being located near Daisy's Garden.
As a result, the majority of Sellbot Cogs reside in the <a href="https://toontown.fandom.com/wiki/Daisy_Gardens#Streets">streets of DG</a>.</p>

<p>I would not have noticed either of these imbalances without charting the samples per street.
Moving forward, we'll be more conscious of street imbalance and take screenshots of Cogs from other streets.</p>

<h3 id="creating-the-dataset-objects">Creating the dataset objects</h3>

<p>We'll create simple tuples of images and labels instead of <code>Keras.dataset</code> objects.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">create_suit_datasets</span><span class="p">(</span>
    <span class="n">split_ratio</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]:</span>
    <span class="sd">&quot;&quot;&quot;Create multiclass Cog suit datasets for training, validation, and testing</span>

<span class="sd">    Args:</span>
<span class="sd">        split_ratio (list[float, float, float], optional): Train/val/test split. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tuple, tuple, tuple]: Train, validate, and test datasets. Each tuple contains</span>
<span class="sd">                                    a numpy array of images and a numpy array of labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">split_ratio</span><span class="p">:</span>
        <span class="n">split_data</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="n">split_ratio</span><span class="p">)</span>

    <span class="n">data_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="n">TRAIN_DIR</span><span class="p">,</span> <span class="n">VALIDATE_DIR</span><span class="p">,</span> <span class="n">TEST_DIR</span><span class="p">]</span>
    <span class="n">suits</span> <span class="o">=</span> <span class="n">get_suits_from_dir</span><span class="p">(</span><span class="n">directories</span><span class="o">=</span><span class="n">data_dirs</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">dir_name</span> <span class="ow">in</span> <span class="n">data_dirs</span><span class="p">:</span>
        <span class="n">filepaths</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">suits</span><span class="p">[</span><span class="n">dir_name</span><span class="p">]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">suit_to_integer</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="p">[</span><span class="n">get_img_array_from_filepath</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span> <span class="k">for</span> <span class="n">fp</span> <span class="ow">in</span> <span class="n">filepaths</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="n">result</span><span class="p">[</span><span class="n">dir_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">TRAIN_DIR</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="n">VALIDATE_DIR</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="n">TEST_DIR</span><span class="p">])</span>
</code></pre></div>

<p>The datasets are now ready to be used by the model.
We can retrieve the datasets, and separate the images and labels, as follows:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Split unsorted images into train, validate, and test sets</span>
<span class="n">ds_train</span><span class="p">,</span> <span class="n">ds_validate</span><span class="p">,</span> <span class="n">ds_test</span> <span class="o">=</span> <span class="n">create_suit_datasets</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="c1"># Create the dataset</span>
<span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">ds_train</span>
<span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="n">ds_validate</span>
<span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">ds_test</span>
</code></pre></div>

<h4 id="splitting-the-images-into-train-validate-and-test">Splitting the images into train, validate, and test</h4>

<hr />

<h2 id="compiling-the-model">Compiling the model</h2>

<h3 id="loss-function">Loss function</h3>

<h3 id="optimizer">Optimizer</h3>

<h3 id="metrics">Metrics</h3>

<h3 id="callbacks">Callbacks</h3>

<h3 id="defining-the-model">Defining the model</h3>

<hr />

<h2 id="training-the-baseline-model">Training the baseline model</h2>

<h3 id="baseline-loss-and-accuracy-plots">Baseline loss and accuracy plots</h3>

<h3 id="baseline-wrong-predictions">Baseline wrong predictions</h3>

<hr />

<h2 id="training-the-optimized-model">Training the optimized model</h2>

<h3 id="keras-tuner">Keras Tuner</h3>

<p><code>KerasTuner</code> is a tool for fine-tuning a model's hyperparameters.
Hyperparameters include the model's layers, layer sizes, and optimizer.
We can leverage this tool to find the best hyperparameters for our model instead of manually tuning the model and comparing the results.</p>

<h3 id="preventing-overfitting">Preventing overfitting</h3>

<h3 id="wrong-predictions">Wrong predictions</h3>

<h3 id="baseline-comparison-training">Baseline comparison: Training</h3>

<h3 id="baseline-comparison-evaluation">Baseline comparison: Evaluation</h3>

<hr />

<h2 id="model-interpretation-and-visualization">Model interpretation and visualization</h2>

<h3 id="intermediate-convnet-outputs-intermediate-activations">Intermediate convnet outputs (intermediate activations)</h3>

<h3 id="convnet-filters">Convnet filters</h3>

<h3 id="class-activation-heatmaps">Class activation heatmaps</h3>

</body>
</html>
