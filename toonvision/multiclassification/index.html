
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../../about">ABOUT</a> &nbsp;<a class="title" href="../../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">August 16, 2022</p>

<p><title>ToonVision: Multiclass Classification</title></p>

<h1 id="toonvision-multiclass-classification">ToonVision - Multiclass Classification</h1>

<p>This article is the second in a series on <strong>ToonVision</strong>.
The <a href="https://fars.io/toonvision/classification/">first article</a> covered the basics of classification and binary classification of Toons and Cogs.
This article covers multiclass classification of Cogs: Cog suits (4 unique suits) and Cog entities (32 unique Cogs).</p>

<p>After reading this article, we'll have a better understanding of how to</p>

<ul>
<li>deal with a model overfitting to a small, imbalanced dataset</li>
<li>utilize image augmentation and dropout to improve the model's generalization capability</li>
<li>compare different models, optimizers, and hyperparameters</li>
<li>automatically optimize hyperparameter values with Keras-tuner</li>
<li>use classification performance measures such as precision, recall and f1-score</li>
<li>interpret and visualize what the model is learning with confusion matrices</li>
</ul>

<p>The following article will cover image segmentation of ToonTown's streets, roads, Cogs, and Cog buildings.
Afterwards, we'll implement real-time object detection and, if possible, image segmentation.
For now, let's focus on multiclass classification.</p>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#toonvision---multiclass-classification">ToonVision - Multiclass Classification</a>
<ul>
<li><a href="#toonvision">ToonVision</a></li>
<li><a href="#classification">Classification</a>
<ul>
<li><a href="#multiclass-classification">Multiclass classification</a></li>
</ul></li>
<li><a href="#the-toonvision-dataset">The ToonVision dataset</a>
<ul>
<li><a href="#dataset-considerations">Dataset considerations</a></li>
<li><a href="#why-does-the-street-matter">Why does the street matter?</a></li>
<li><a href="#dataset-balance">Dataset balance</a>
<ul>
<li><a href="#why-does-the-brrrgh-have-the-most-bossbot-samples">Why does The Brrrgh have the most Bossbot samples?</a></li>
<li><a href="#why-are-there-so-many-lawbot-and-sellbot-samples-in-daisys-garden">Why are there so many Lawbot and Sellbot samples in Daisy's Garden?</a></li>
</ul></li>
<li><a href="#creating-the-datasets">Creating the datasets</a>
<ul>
<li><a href="#splitting-images-into-train-validate-and-test-sets">Splitting images into train, validate, and test sets</a></li>
</ul></li>
</ul></li>
<li><a href="#compiling-the-model">Compiling the model</a>
<ul>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#metrics">Metrics</a>
<ul>
<li><a href="#accuracy">Accuracy</a></li>
<li><a href="#precision">Precision</a></li>
<li><a href="#recall">Recall</a></li>
</ul></li>
<li><a href="#optimizer">Optimizer</a></li>
<li><a href="#callbacks">Callbacks</a></li>
<li><a href="#defining-the-model">Defining the model</a></li>
</ul></li>
<li><a href="#training-the-baseline-model">Training the baseline model</a>
<ul>
<li><a href="#baseline-loss-and-accuracy-plots">Baseline loss and accuracy plots</a></li>
<li><a href="#baseline-wrong-predictions">Baseline wrong predictions</a></li>
</ul></li>
<li><a href="#training-the-optimized-model">Training the optimized model</a>
<ul>
<li><a href="#keras-tuner">Keras Tuner</a></li>
<li><a href="#preventing-overfitting">Preventing overfitting</a></li>
<li><a href="#wrong-predictions">Wrong predictions</a></li>
<li><a href="#baseline-comparison-training">Baseline comparison: Training</a></li>
<li><a href="#baseline-comparison-evaluation">Baseline comparison: Evaluation</a></li>
</ul></li>
<li><a href="#model-interpretation-and-visualization">Model interpretation and visualization</a>
<ul>
<li><a href="#intermediate-convnet-outputs-intermediate-activations">Intermediate convnet outputs (intermediate activations)</a></li>
<li><a href="#convnet-filters">Convnet filters</a></li>
<li><a href="#class-activation-heatmaps">Class activation heatmaps</a></li>
</ul></li>
<li><a href="#references">References</a>
</details></li>
</ul></li>
</ul>

<h2 id="toonvision">ToonVision</h2>

<p>ToonVision is my computer vision project for teaching a machine how to see in <a href="https://en.wikipedia.org/wiki/Toontown_Online">ToonTown Online</a> - an MMORPG created by Disney in 2003.
The ultimate goal is to teach a machine (nicknamed <strong>OmniToon</strong>) how to play ToonTown and create a self-sustaining ecosystem where the bots progress through the game together.</p>

<hr />

<h2 id="classification">Classification</h2>

<p>As discussed in the <a href="https://fars.io/toonvision/classification/#classification">previous article</a>, image classification is the process of assigning a label to an input image.
For instance, given a dog-vs-cat classification model and an image of a Pomeranian, the model will predict that the image is a dog.</p>

<p>There are a few variants of the image classification problem: binary, multiclass, multi-label, and so on.
We'll focus on <a href="https://fars.io/toonvision/classification/#multiclass-classification">multiclass classification</a> in this article.</p>

<h3 id="multiclass-classification">Multiclass classification</h3>

<p>Multiclass classification is a problem in which the model predicts which class an input image belongs.
For instance, the model could predict that an animal belongs to the class of dogs, cats, rabbits, horses, or any other animal.</p>

<p>We're building a model to predict which Cog suit a Cog belongs to - a 4-class classification problem.
We'll push the model further by also predicting which Cog entity a Cog belongs to - a 32-class classification problem.
Both multiclass classification problems require us to improve the ToonVision dataset.
The current dataset is imbalanced and does not contain samples of all the Cog suits and entities.
Let's look at how we can improve the dataset in the next section.</p>

<hr />

<h2 id="the-toonvision-dataset">The ToonVision dataset</h2>

<h3 id="dataset-considerations">Dataset considerations</h3>

<p>We'll tweak the existing dataset considerations to focus on balancing the dataset's Cog entity samples.
The current Cog dataset contains two glaring problems:</p>

<ol>
<li>The samples do not account for <em>where</em> - on <em>which street</em> - the Cog is located.</li>
<li>There are no samples of the two highest-ranked Cogs</li>
</ol>

<p>To address problem #1, the ToonVision dataset now requires a balanced number of samples from each of the 6 uniquely designed streets: The Brrrgh, Daisy's Garden, Donald's Dreamland, Donald's Dock, Minnie's Melodyland, and ToonTown Central.
Given that there are 6 streets, the sample images per Cog will increase from 20 to 30; we need 5 images of a Cog from each street.</p>

<p>We'll leverage Cog invasions to ensure we meet the new dataset requirement because some Cogs are not present in certain streets <em>unless</em> there's an ongoing invasion.
Cog invasions will solve both problems moving forward.</p>

<h3 id="why-does-the-street-matter">Why does the street matter?</h3>

<p>Each street in ToonTown has a unique design - colors, houses, floors, obstacles - resulting in unique backgrounds for our extracted objects.
In this case, our extracted objects are Cogs.
If we have a diverse dataset of Cogs with different backgrounds, we can teach our model to better recognize Cog features rather than background features.
Therefore, it's important to take screenshots of Cogs from each street so our model can generalize Cog features across all streets.</p>

<h3 id="dataset-balance">Dataset balance</h3>

<p>The Cog vs Toon dataset is imbalanced with a majority of the dataset belonging to the Cog class.
However, the Cog dataset is mostly balanced.
Our goal is ~30 samples per Cog entity with at least 5 samples per street.
We have achieved the 30 samples per Cog entity requirement, but we're not meeting the 5 samples per street requirement.</p>

<figure class="center" style="width:95%">
    <img src="img/dataset_streets.png" style="width:100%;"/>
    <figcaption>Dataset balance per street</figcaption>
</figure>

<p>There image above shows two notable imbalances.
In the Suits per street graph, we can see:</p>

<ol>
<li>Daisy's Garden (DG) has the most Lawbot and Sellbot samples.</li>
<li>The Brrrgh (BR) has the most Bossbot samples.</li>
</ol>

<h4 id="why-does-the-brrrgh-have-the-most-bossbot-samples">Why does The Brrrgh have the most Bossbot samples?</h4>

<p>The street I frequented to acquire samples in The Brrrgh was <a href="https://toontown.fandom.com/wiki/Walrus_Way">Walrus Way</a>, where the Cog presence is split [90%, 10%] between Bossbots and Lawbots, respectively.
It's no surprise that the majority of the samples from the Brrrgh street are Bossbot samples.</p>

<h4 id="why-are-there-so-many-lawbot-and-sellbot-samples-in-daisys-garden">Why are there so many Lawbot and Sellbot samples in Daisy's Garden?</h4>

<p>The Lawbot imbalance is because the street I visited was majority Lawbot and I took too many screenshots of Bottom Feeders.
The Sellbot imbalance, on the other hand, is due to the Sellbot HQ being located near Daisy's Garden.
As a result, the majority of Sellbot Cogs reside in the <a href="https://toontown.fandom.com/wiki/Daisy_Gardens#Streets">streets of DG</a>.</p>

<p>I would not have noticed either of these imbalances without charting the samples per street.
Moving forward, we'll be more conscious of street imbalance and take screenshots of Cogs from other streets.</p>

<h3 id="creating-the-datasets">Creating the datasets</h3>

<p>We'll create simple tuples of images and labels instead of <code>Keras.dataset</code> objects.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">create_suit_datasets</span><span class="p">(</span>
    <span class="n">split_ratio</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]:</span>
    <span class="sd">&quot;&quot;&quot;Create multiclass Cog suit datasets for training, validation, and testing</span>

<span class="sd">    Args:</span>
<span class="sd">        split_ratio (list[float, float, float], optional): Train/val/test split. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tuple, tuple, tuple]: Train, validate, and test datasets. Each tuple contains</span>
<span class="sd">                                    a numpy array of images and a numpy array of labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">split_ratio</span><span class="p">:</span>
        <span class="n">split_data</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="n">split_ratio</span><span class="p">)</span>

    <span class="n">data_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="n">TRAIN_DIR</span><span class="p">,</span> <span class="n">VALIDATE_DIR</span><span class="p">,</span> <span class="n">TEST_DIR</span><span class="p">]</span>
    <span class="n">suits</span> <span class="o">=</span> <span class="n">get_suits_from_dir</span><span class="p">(</span><span class="n">directories</span><span class="o">=</span><span class="n">data_dirs</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">dir_name</span> <span class="ow">in</span> <span class="n">data_dirs</span><span class="p">:</span>
        <span class="n">filepaths</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">suits</span><span class="p">[</span><span class="n">dir_name</span><span class="p">]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">suit_to_integer</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="p">[</span><span class="n">get_img_array_from_filepath</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span> <span class="k">for</span> <span class="n">fp</span> <span class="ow">in</span> <span class="n">filepaths</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="n">result</span><span class="p">[</span><span class="n">dir_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">TRAIN_DIR</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="n">VALIDATE_DIR</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="n">TEST_DIR</span><span class="p">])</span>
</code></pre></div>

<p>The datasets are now ready to be used by the model.
We can retrieve the datasets, and separate the images and labels, as follows:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Split unsorted images into train, validate, and test sets</span>
<span class="n">ds_train</span><span class="p">,</span> <span class="n">ds_validate</span><span class="p">,</span> <span class="n">ds_test</span> <span class="o">=</span> <span class="n">create_suit_datasets</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="c1"># Create the dataset</span>
<span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">ds_train</span>
<span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="n">ds_validate</span>
<span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">ds_test</span>
</code></pre></div>

<h4 id="splitting-images-into-train-validate-and-test-sets">Splitting images into train, validate, and test sets</h4>

<p>Previously, we split the entire dataset into 60%/20%/20% train/validate/test sets.
This resulted in unbalanced suit samples in each set.
For instance, the Bossbot suit samples would contain a disproportionate number of sample from Flunkies relative to other Bossbots.</p>

<p>To maintain balanced datasets for each Cog suit, we split each individual Cog entity using the 60/20/20 split.
This ensures a representative sample of each Cog entity.</p>

<hr />

<h2 id="compiling-the-model">Compiling the model</h2>

<p>Now that we've created the datasets, we can compile the model.
Compiling the model requires choosing a loss function, optimizer, and metrics to monitor the model's performance during training.</p>

<h3 id="loss-function">Loss function</h3>

<p>Our model is classifying multiple classes, so we'll have to choose between <code>categorical_crossentropy</code> or <code>sparse_categorical_crossentropy</code>.
There's one small, but important difference between the two loss functions: <code>categorical_crossentropy</code> requires one-hot encoded labels whereas <code>sparse_categorical_crossentropy</code> requires integer labels.</p>

<p>Given that our labels are one-hot encoded, we'll utilize the <code>categorical_crossentropy</code> loss function during our model's training.
The decision to one-hot encode our labels stemmed from the desire to utilize <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision">precision</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall">recall</a> metrics.
More on these metrics in the following section.</p>

<h3 id="metrics">Metrics</h3>

<p>We'll track three metrics during training: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy">categorical_accuracy</a>, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision">precision</a>, and <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall">recall</a>.
These metrics measure the model's correctness and quality.
The differences between each metric is crucial to understanding model performance - especially on imbalanced datasets like the ToonVision dataset.
To read more about the differences and benefits of each metric, please read my previous article about <a href="https://fars.io/performance_classification/">performance measures in classification problems</a>.</p>

<h4 id="accuracy">Accuracy</h4>

<p>Accuracy is the percentage of correct predictions.
Keras contains two built-in metrics for multiclass classification tasks: <code>categorical_accuracy</code> for categorical labels (one-hot encoded) and <code>sparse_categorical_accuracy</code> for integer labels.</p>

<p><code>categorical_accuracy</code> is best used as a performance measure when working with balanced datasets.
It's a misleading metric when used with imbalanced datasets because it cannot account for class imbalances.
Unfortunately for us, the ToonVision dataset is imbalanced, so we must find other metrics to work with.</p>

<h4 id="precision">Precision</h4>

<p>Precision answers the question of "what proportion of predicted class labels actually belong to the predicted class?".
To make the question more relevant to our problem, "what proportion of predicted Bossbot labels actually belong to the Bossbot class?".</p>

<p>We often refer to precision as the model's reliability.
When precision is high, we can trust the model to correctly classify a class.</p>

<h4 id="recall">Recall</h4>

<p>Recall answers the question of "what proportion of a specific label was correctly identified?".
Relating to our ToonVision dataset, "what proportion of Bossbot labels were correctly identified?".</p>

<p>Recall is a powerful performance measure when working with highly imbalanced datasets.
It shines at identifying rare classes.
For instance, imagine if our ToonVision dataset contained 400 Bossbot samples and only 40 Sellbot samples.
Optimizing our model's Sellbot recall would ensure that Sellbot samples do not get incorrectly labeled as something else.</p>

<h3 id="optimizer">Optimizer</h3>

<h3 id="callbacks">Callbacks</h3>

<h3 id="defining-the-model">Defining the model</h3>

<hr />

<h2 id="training-the-baseline-model">Training the baseline model</h2>

<h3 id="baseline-loss-and-accuracy-plots">Baseline loss and accuracy plots</h3>

<h3 id="baseline-wrong-predictions">Baseline wrong predictions</h3>

<hr />

<h2 id="training-the-optimized-model">Training the optimized model</h2>

<h3 id="keras-tuner">Keras Tuner</h3>

<p><code>KerasTuner</code> is a tool for fine-tuning a model's hyperparameters.
Hyperparameters include the model's layers, layer sizes, and optimizer.
We can leverage this tool to find the best hyperparameters for our model instead of manually tuning the model and comparing the results.</p>

<h3 id="preventing-overfitting">Preventing overfitting</h3>

<h3 id="wrong-predictions">Wrong predictions</h3>

<h3 id="baseline-comparison-training">Baseline comparison: Training</h3>

<h3 id="baseline-comparison-evaluation">Baseline comparison: Evaluation</h3>

<hr />

<h2 id="model-interpretation-and-visualization">Model interpretation and visualization</h2>

<h3 id="intermediate-convnet-outputs-intermediate-activations">Intermediate convnet outputs (intermediate activations)</h3>

<h3 id="convnet-filters">Convnet filters</h3>

<h3 id="class-activation-heatmaps">Class activation heatmaps</h3>

<hr />

<h2 id="references">References</h2>

</body>
</html>
