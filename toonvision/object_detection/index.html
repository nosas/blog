
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../../about">ABOUT</a> &nbsp;<a class="title" href="../../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">September 14, 2022</p>

<p><title>ToonVision: Object Detection</title></p>

<h1 id="toonvision-object-detection">ToonVision - Object Detection</h1>

<p>This article is the third in a series on <strong>ToonVision</strong>.
The <a href="https://fars.io/toonvision/classification/">first article</a> covered the basics of classification and binary classification of Toons and Cogs.
More specifically, the previous article covered how to...</p>

<ul>
<li>convert a binary dataset into a multiclass dataset</li>
<li>use classification performance measures such as precision, recall, and F1-score</li>
<li>automatically optimize hyperparameter values with Keras-tuner</li>
<li>interpret and visualize what the model is learning with confusion matrices and class activation maps</li>
</ul>

<p>This article explains object detection of Cogs - also called entity detection.
After reading this article, you should have a better understanding of how to...</p>

<ul>
<li>differentiate between object detection models (YOLO, R-CNN, SSD)</li>
<li>create an object detection model</li>
<li>extract objects from images and videos</li>
<li>build data pipelines to semi-autonomously grow a dataset (semi-supervised learning)</li>
</ul>

<p>The next article will cover image segmentation of ToonTown's streets, roads, Toons, Cogs, and Cog buildings.
For now, let's focus on object detection.</p>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#toonvision---object-detection">ToonVision - Object Detection</a>
<ul>
<li><a href="#toonvision">ToonVision</a></li>
<li><a href="#object-detection">Object detection</a>
<ul>
<li><a href="#object-detection-models">Object detection models</a>
<ul>
<li><a href="#two-shot">Two-shot</a></li>
<li><a href="#single-shot">Single-shot</a></li>
</ul></li>
<li><a href="#r-cnn">R-CNN</a></li>
<li><a href="#ssd">SSD</a></li>
<li><a href="#yolo">YOLO</a></li>
</ul></li>
<li><a href="#creating-an-object-detection-model">Creating an object detection model</a>
<ul>
<li><a href="#issues-encountered">Issues encountered</a>
<ul>
<li><a href="#input-image-size">Input image size</a></li>
<li><a href="#accuracy-or-speed">Accuracy or speed</a></li>
<li><a href="#unable-to-detect-toons">Unable to detect Toons</a></li>
<li><a href="#dataset-inconsistencies">Dataset inconsistencies</a></li>
</ul></li>
</ul></li>
<li><a href="#save-predicted-annotations-in-pascal-voc-format">Save predicted annotations in PASCAL VOC format</a>
<ul>
<li><a href="#extract-the-annotated-objects">Extract the annotated objects</a></li>
</ul></li>
<li><a href="#build-data-pipelines-to-semi-autonomously-grow-a-dataset">Build data pipelines to semi-autonomously grow a dataset</a></li>
<li><a href="#references">References</a>
</details></li>
</ul></li>
</ul>

<h2 id="toonvision">ToonVision</h2>

<p>ToonVision is my computer vision project for teaching a machine how to see in <a href="https://en.wikipedia.org/wiki/Toontown_Online">ToonTown Online</a> - an MMORPG created by Disney in 2003.
The project's objective is to teach a machine (nicknamed <strong>OmniToon</strong>) how to play ToonTown and create a self-sustaining ecosystem where the bots progress through the game together.
In the process, I will explain the intricacies of building computer vision models, configuring static and real-time (stream) data pipelines, and visualizing results and progress.</p>

<hr />

<h2 id="object-detection">Object detection</h2>

<p>Object detection is a computer vision task of detecting instances of objects from pre-defined classes in images and videos.
Specifically, object detection models detect, label, and draw a bounding box around each object.</p>

<p><font style="color:red">TODO: Include row of ToonVision images: classification, classification + localization, object detection, instance segmentation</font></p>

<p>Common use-cases include face detection in cameras and pedestrian detection in autonomous vehicles.
In ToonVision's, object detection is applied to locating all entities - Cogs and Toons - in both images and real-time video.</p>

<h3 id="object-detection-models">Object detection models</h3>

<p>The most popular object detection models can be split into two main groups: single-shot and two-shot.
Each group has accuracy and speed tradeoffs.
Single-shot models excel in tasks requiring high detection speed, such as in real-time videos.
Two-shot models are slower but more accurate; therefore, they're primarily used in tasks involving image data or low-FPS videos.</p>

<h4 id="two-shot">Two-shot</h4>

<p>Two-shot detection models have two stages: region proposal and then classification of those regions and refinement of the location prediction.
The two steps require significant computational resources, resulting in slow training and inference.</p>

<p><font style="color:red">TODO: Insert image of ToonTown region proposal -> classification and regression networks</font></p>

<p>Despite the slowness, two-shot models have far superior accuracy when compared to single-shot models.
R-CNN<sup>[1]</sup> is a commonly used two-shot detection model.
Faster R-CNN<sup>[2]</sup>, R-CNN's improved variant, is the more popular choice for two-shot models.</p>

<h4 id="single-shot">Single-shot</h4>

<p>Single-shot models are designed for real-time object detection.
They have quicker inference speeds and use less resources during training than two-shot models.
These two properties allow for quick training, prototyping, and experimenting without consuming considerable computation resources.</p>

<p><font style="color:red">TODO: Insert image of anchor boxes and feature maps</font></p>

<p>In a single forward pass, these models predict bounding boxes and class labels directly from the input's feature maps.
They skip the region proposal stage and yield final localization and content prediction at once.
SSD<sup>[3]</sup> and YOLO<sup>[4]</sup> are popular single-shot object detection models capable of running at 5-160 frames per second!</p>

<h3 id="r-cnn">R-CNN</h3>

<p>Regions with Convolutional Neural Networks (R-CNN) is a two-shot detection algorithm created in 2013.
R-CNN combines <strong>rectangular region proposals</strong> with <strong>convolutional neural network features</strong> to detect objects.
The first stage, region proposal, identifies a subset of regions in an image that might contain an object.
The second stage classifies the object in each region using a CNN classifier.
Each proposed object requires a forward pass of the classification network; as a result, the algorithm has slow inference speed.</p>

<p><font style="color:red">TODO: Insert image showing bounding boxes, different region proposals, and result</font></p>

<p>R-CNNs can be boiled down to the following three processes:</p>

<ol>
<li>Find regions in the image that might contain an object (region proposals)</li>
<li>Extract features from the region proposals</li>
<li>Classify the objects using the extracted features</li>
</ol>

<p>There are many variants of R-CNN: Fast R-CNN<sup>[5]</sup>, Faster R-CNN<sup>[2]</sup>, Mask R-CNN<sup>[6]</sup>.
Each variant improves performance, but the algorithm is still slow when compared to single-shot.
Mask R-CNN is unique because it's used for image segmentation while all others are for object detection.</p>

<p>R-The CNN algorithm has incredible accuracy in low-FPS or still-image tasks.
However, both SSD and YOLO significantly outperform R-CNN in real-time object detection.</p>

<h3 id="ssd">SSD</h3>

<p>Developed in 2015, the Single Shot MultiBox Detector (SSD) is a method for detecting object in images using a single deep neural network.
Boxes of different aspect ratios and scales overlay each feature map.
At prediction time, the network generates scores for the presence of each object in each box.
The network combines predictions from multiple feature maps with different resolutions to handle objects of various sizes.</p>

<p><font style="color:red">TODO: Insert image showing bounding boxes, different ratio boxes, and result</font></p>

<p>Like all other single-shot models, SSD eliminates proposal generation and feature resampling.
All computation is done in a single network, making SSD easy to train and integrate into systems requiring a detection component.</p>

<p>Although SSD was state-of-the-art when it came out in 2015, there's a new king in town: YOLO.</p>

<h3 id="yolo">YOLO</h3>

<p>The You Only Look Once (YOLO) model is a new approach to unified, real-time object detection.
Created in 2015 by Joseph Redmon and gang, YOLO reframes object detection as a regression problem rather than leveraging regional proposals and a CNN classifier.
Like all single-shot algorithms, YOLO's single neural network predicts bounding boxes and class probabilities from images in one pass.</p>

<p><font style="color:red">TODO: Insert image showing bounding boxes, different ratio boxes, and result</font></p>

<!-- The YOLO framework has three main components:

- Backbone
- Neck
- Head

The **Backbone** mainly extracts essential features of an image and feeds them to the Head through Neck.
The **Neck** collects feature maps extracted by the Backbone and creates feature pyramids.
Finally, the **Head** consists of output layers that have final detections. -->

<p>YOLO is insanely fast.
The base model processes images in real-time at 45 FPS.
A smaller version, Fast YOLO, processes an astounding 155 FPS!
Take a peek at YOLOv3's performance from the author's own 3-minute <a href="https://www.youtube.com/watch?v=MPU2HistivI">YouTube video</a>.</p>

<p>Where YOLO excels in speed, it struggles in accuracy.
The algorithm is prone to making localization errors (sizes and location of bounding boxes).
When comparing to state-of-the-art detection systems, however, YOLO is far less likely to predict false detections where nothing exists.</p>

<p>There have been <strong>seven</strong> iterations on the algorithm since its inception in 2015.
Each variation resulted in higher accuracy and inference speed.
The newest version, YOLOv7<sup>[7]</sup>, was released in July 2022 and is capable of 160FPS.</p>

<hr />

<h2 id="creating-an-object-detection-model">Creating an object detection model</h2>

<p>I'll leverage <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md">TensorFlow's model zoo</a> to fine-tune and train a SSD model.
This article will explain the general procedures for building the model.
A rough step-by-step page can be found in my knowledge base.
In the future, I'd like to write an article about how feature extractors are created and used for smaller projects like this.</p>

<h3 id="issues-encountered">Issues encountered</h3>

<p>I encountered many issues that are not often discussed in articles and tutorials.
In an effort to provide a realistic overview of my process, I've explained the following issues:</p>

<ol>
<li>Input image size being too large</li>
<li>Accuracy or speed for model selection</li>
<li>Model does not detect Toons</li>
<li>Bounding box inconsistencies in the dataset</li>
</ol>

<h4 id="input-image-size">Input image size</h4>

<p>My main concern about this project revolved around the dataset's image sizes being too large at 3440x1440.
I trained a larger model (Faster R-CNN ResNet152) on the dataset and each training step took over 3 seconds.
It took over 90 minutes to train 1000 steps!
Even worse, the model likely would not converge until the following day; so, I stopped training.</p>

<p>Scaling the image sizes down by half (1720x720) resulted in faster training and inference speed.
Further scaling the images down to 1/4 the original size (860x360) led to even faster training.
The model converged in less than an hour after 50,000 training steps.
Issue #1 resolved!</p>

<h4 id="accuracy-or-speed">Accuracy or speed</h4>

<p>I had originally planned to have two model: one for real-time detection in videos (speed) and another for detection in images (accuracy).
Faster R-CNN was to be used in image detection because I wanted accuracy for the data pipeline.
SSD or YOLO for the real-time video detection.
Given the large size and lengthy training process of two-shot models, I've scrapped the idea of two models in favor of a single SSD model.
Issue #2 resolved!</p>

<h4 id="unable-to-detect-toons">Unable to detect Toons</h4>

<p>The trained SSD model does not detect Toons.
In fact, the Toons it does detect are classified as Cogs.
This issue is largely due to the massive dataset imbalance: 526 Cog samples and 148 Toon samples.
There are two solutions: Increase weights of Toon localization and classification during training or modify entire dataset to include only Cogs.</p>

<p>Recall that the goal of ToonVision is for a Toon to see Cogs.
Given that it's more important to detect Cogs, I will exclude Toons from the dataset.
I generated new TensorFlow record files which consist purely of Cogs and excluded any images that contained only Toons.
Issue #3 resolved!</p>

<h4 id="dataset-inconsistencies">Dataset inconsistencies</h4>

<p>This is probably an uncommon issue.
When creating the binary and multi-class classification models, I opted to only include clear, non-obstructed Cog and Toon samples.
This means I did not put bounding boxes on entities that were occluded by another object.
However, the trained SSD model detects and classifies occluded Cogs!</p>

<p><font style="color:red">TODO: Insert image of sample vs predictions</font></p>

<p><em>Why is it bad for the model to detect objects that I did not classify in the training set?</em>
It negatively affects the training loss.
More specifically, the localization loss increases during training.</p>

<p><em>What's causing the loss increase?</em>
During training, the localization loss is calculated in part by how accurate the model's predicted bounding boxes compares to the ground truth bounding boxes.
In object detection terms, the bounding box accuracy is called the <strong>Intersection over Union</strong> (IoU).
IoU scores the overlap of the predicted box and the ground truth box.
The higher the IoU score, the higher the accuracy.
If there's no ground truth box, however, the IoU score will be zero and training loss will increase.</p>

<p><font style="color:red">TODO: Insert image of IoU</font></p>

<p><em>How does this inconsistency affect training?</em>
The inconsistency did not affect the model's performance, but it decreased training performance and convergence.</p>

<p><font style="color:red">TODO: Insert Tensorboard loss graphs</font></p>

<p><em>How can I resolve the issue?</em>
I would have to go through the dataset and label the non-labeled Cogs.
Alternatively, I could run the entire dataset through the model and compare the number of detected objects against the ground truth.
If there's a discrepancy, I can manually review the ground truth labels and correct them if needed.</p>

<p>In short, the model localizes objects that I did not declare as ground truth.
It slowed down training but did not affect performance too much.
Issue #4 explained, but unresolved!</p>

<hr />

<h2 id="save-predicted-annotations-in-pascal-voc-format">Save predicted annotations in PASCAL VOC format</h2>

<h3 id="extract-the-annotated-objects">Extract the annotated objects</h3>

<hr />

<h2 id="build-data-pipelines-to-semi-autonomously-grow-a-dataset">Build data pipelines to semi-autonomously grow a dataset</h2>

<hr />

<h2 id="references">References</h2>

<ol>
<li><p>Rich feature hierarchies for accurate object detection and semantic segmentation, <a href="https://arxiv.org/abs/1311.2524">https://arxiv.org/abs/1311.2524</a></p></li>
<li><p>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, <a href="https://arxiv.org/abs/1506.01497">https://arxiv.org/abs/1506.01497</a></p></li>
<li><p>SSD: Single Shot MultiBox Detector, <a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a></p></li>
<li><p>You Only Look Once: Unified, Real-Time Object Detection, <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></p></li>
<li><p>Fast R-CNN, <a href="https://arxiv.org/abs/1504.08083">https://arxiv.org/abs/1504.08083</a></p></li>
<li><p>Mask R-CNN, <a href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a></p></li>
<li><p>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, <a href="https://arxiv.org/abs/2207.02696">https://arxiv.org/abs/2207.02696</a></p></li>
</ol>

</body>
</html>
