
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../../about">ABOUT</a> &nbsp;<a class="title" href="../../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">August 31, 2022</p>

<p><title>ToonVision: Object detection</title></p>

<h1 id="toonvision-object-detection">ToonVision - Object Detection</h1>

<p>This article is the third in a series on <strong>ToonVision</strong>.
The <a href="https://fars.io/toonvision/classification/">first article</a> covered the basics of classification and binary classification of Toons and Cogs.
More specifically, the previous article covered how to...</p>

<ul>
<li>convert a binary dataset into a multiclass dataset</li>
<li>use classification performance measures such as precision, recall, and F1-score</li>
<li>automatically optimize hyperparameter values with Keras-tuner</li>
<li>interpret and visualize what the model is learning with confusion matrices and class activation maps</li>
</ul>

<p>This article explains object detection of Cogs and Toons - also called entity detection.
After reading this article, you should have a better understanding of how to...</p>

<ul>
<li>differentiate between object detection models (YOLO, R-CNN, SSD)</li>
<li>create an object detection model</li>
<li>extract detected objects from images and videos</li>
<li>build data pipelines to semi-autonomously grow a dataset (semi-supervised learning)</li>
</ul>

<p>The next article will cover image segmentation of ToonTown's streets, roads, Toons, Cogs, and Cog buildings.
For now, let's focus on object detection.</p>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#toonvision---object-detection">ToonVision - Object Detection</a>
<ul>
<li><a href="#toonvision">ToonVision</a></li>
<li><a href="#object-detection">Object detection</a>
<ul>
<li><a href="#object-detection-models">Object detection models</a>
<ul>
<li><a href="#two-shot">Two-shot</a></li>
<li><a href="#single-shot">Single-shot</a></li>
</ul></li>
<li><a href="#r-cnn">R-CNN</a></li>
<li><a href="#ssd">SSD</a></li>
<li><a href="#yolo">YOLO</a></li>
</ul></li>
<li><a href="#creating-an-object-detection-model">Creating an object detection model</a></li>
<li><a href="#references">References</a>
</details></li>
</ul></li>
</ul>

<h2 id="toonvision">ToonVision</h2>

<p>ToonVision is my computer vision project for teaching a machine how to see in <a href="https://en.wikipedia.org/wiki/Toontown_Online">ToonTown Online</a> - an MMORPG created by Disney in 2003.
The project's objective is to teach a machine (nicknamed <strong>OmniToon</strong>) how to play ToonTown and create a self-sustaining ecosystem where the bots progress through the game together.
In the process, I will explain the intricacies of building computer vision models, configuring static and real-time (stream) data pipelines, and visualizing results and progress.</p>

<hr />

<h2 id="object-detection">Object detection</h2>

<p>Object detection is a computer vision task of detecting instances of objects from pre-defined classes in images and videos.
Specifically, object detection models detect, label, and draw a bounding box around each object.</p>

<p><font style="color:red">TODO: Include row of ToonVision images: classification, classification + localization, object detection, instance segmentation</font></p>

<p>Common use-cases include face detection in cameras and pedestrian detection in autonomous vehicles.
In ToonVision's, object detection is applied to locating all entities - Cogs and Toons - in both images and real-time video.</p>

<h3 id="object-detection-models">Object detection models</h3>

<p>The most popular object detection models can be split into two main groups: single-shot and two-shot.
Each group has accuracy and speed tradeoffs.
Single-shot models excel in tasks requiring high detection speed, such as in real-time videos.
Two-shot models are slower but more accurate; therefore, they're primarily used in tasks involving image data or low-FPS videos.</p>

<h4 id="two-shot">Two-shot</h4>

<p>Two-shot detection models have two stages: region proposal and then classification of those regions and refinement of the location prediction.
The two steps require significant computational resources, resulting in slow training and inference.</p>

<p>Despite the slowness, two-shot models have far superior accuracy when compared to single-shot models.
R-CNN<sup>[1]</sup> is a commonly used two-shot detection model.
Faster R-CNN<sup>[2]</sup>, R-CNN's improved variant, is the more popular choice for two-shot models.</p>

<h4 id="single-shot">Single-shot</h4>

<p>Single-shot models are designed for real-time object detection.
They have quicker inference speeds and use less resources during training than two-shot models.
These two properties allow for quick training, prototyping, and experimenting without consuming considerable computation resources.</p>

<p>In a single forward pass, these models predict bounding boxes and class labels directly from the input's feature maps.
They skip the region proposal stage and yield final localization and content prediction at once.
SSD<sup>[3]</sup> and YOLO<sup>[4]</sup> are popular single-shot object detection models capable of running at 5-160 frames per second!</p>

<h3 id="r-cnn">R-CNN</h3>

<p>Regions with convolutional neural networks (R-CNN) is a two-shot detection algorithm.
R-CNN combines rectangular region proposals with convolutional neural network features to detect objects.
The first stage, region proposal, identifies a subset of regions in an image that might contain an object.
The second stage classifies the object in each region.</p>

<p>R-CNNs can be boiled down to the following three processes:</p>

<ol>
<li>Find regions in the image that might contain an object (region proposals)</li>
<li>Extract features from the region proposals</li>
<li>Classify the objects using the extracted features</li>
</ol>

<p>2014, many variations (Fast R-CNN, Faster R-CNN, Mask R-CNN), slow but accurate</p>

<h3 id="ssd">SSD</h3>

<p>Single-shot algorithm
2015, SOTA at the time, but no longer. YOLO is king</p>

<h3 id="yolo">YOLO</h3>

<p>Single-shot algorithm
2015, many different versions v1 -> v7
2022, YOLOv7 newest version, 5-160 FPS<sup>[5]</sup>
Faster than SSD, although less accurate.</p>

<hr />

<h2 id="creating-an-object-detection-model">Creating an object detection model</h2>

<hr />

<h2 id="references">References</h2>

<ol>
<li><p>Rich feature hierarchies for accurate object detection and semantic segmentation, <a href="https://arxiv.org/abs/1311.2524">https://arxiv.org/abs/1311.2524</a></p></li>
<li><p>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, <a href="https://arxiv.org/abs/1506.01497">https://arxiv.org/abs/1506.01497</a></p></li>
<li><p>SSD: Single Shot MultiBox Detector, <a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a></p></li>
<li><p>You Only Look Once: Unified, Real-Time Object Detection, <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></p></li>
<li><p>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, <a href="https://arxiv.org/abs/2207.02696">https://arxiv.org/abs/2207.02696</a></p></li>
</ol>

</body>
</html>
