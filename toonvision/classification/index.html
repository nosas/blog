
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../../css/default_dark.css">
  <link rel="stylesheet" type="text/css" href="../../css/syntax_dark.css">
</head>

<body>
  <center>
    <div style="display: inline-block; vertical-align:middle;">
      <a href="/" style="text-decoration: none;">SASON REZA<br>
      </a>
      <hr>
      <div style="text-align: center;display: inline-block; width: 100%;">
        <a class="title" href="../../about">ABOUT</a> &nbsp;<a class="title" href="../../contact">CONTACT</a>
      </div>
    </div>
  </center>

  <br>
  <p style="margin-bottom: 2ch;text-align: right;font-style: italic;">June 19, 2022</p>

<p><title>ToonVision: Binary and Multiclass Classification</title></p>

<h1 id="toonvision-classification">ToonVision - Classification</h1>

<p>This article is first in a series on <strong>ToonVision</strong>.</p>

<p>ToonVision is my personal computer vision project for teaching a machine how to see in <a href="https://en.wikipedia.org/wiki/Toontown_Online">ToonTown Online</a> - an MMORPG created by Disney in 2002.
The ultimate goal is to teach a machine (nicknamed <strong>OmniToon</strong>) how to play ToonTown and create a self-sustaining ecosystem within the game where the bots progress through the game together.</p>

<p>In later articles, we'll dive into image segmentation and object detection.
For now, let's focus on real-time classification.</p>

<p>This article covers ...</p>

<ul>
<li>Binary classification: Toon vs Cog</li>
<li>Multiclass classification: Cog suits (4 unique suits) and Cog names (32 unique names)</li>
</ul>

<p>After reading this article, we'll have a better understanding of...</p>

<ul>
<li>How to acquire, label, and process samples from image data</li>
<li>How to deal with a model overfitting to a small, imbalanced dataset</li>
<li>How to utilize image augmentation and dropout to improve the model's generalization capability</li>
<li>How to compare different models, optimizers, and hyperparameters</li>
</ul>

<p><details>
    <summary>Table of Contents</summary></p>

<ul>
<li><a href="#toonvision---classification">ToonVision - Classification</a>
<ul>
<li><a href="#classification">Classification</a>
<ul>
<li><a href="#binary-classification">Binary classification</a></li>
<li><a href="#multiclass-classification">Multiclass classification</a>
<ul>
<li><a href="#multiclass-multilabel-classification">Multiclass multilabel classification</a></li>
</ul></li>
</ul></li>
<li><a href="#toontown">ToonTown</a>
<ul>
<li><a href="#toon">Toon</a></li>
<li><a href="#cog">Cog</a></li>
<li><a href="#why-is-it-important-for-toons-to-classify-cogs">Why is it important for Toons to classify Cogs?</a></li>
</ul></li>
<li><a href="#the-toonvision-dataset">The ToonVision dataset</a>
<ul>
<li><a href="#dataset-considerations">Dataset considerations</a></li>
<li><a href="#filename-and-data-folder-structure">Filename and data folder structure</a></li>
<li><a href="#data-acquisition">Data acquisition</a>
<ul>
<li><a href="#can-we-use-gans-to-synthesize-additional-data">Can we use GANs to synthesize additional data?</a></li>
</ul></li>
<li><a href="#data-labeling">Data labeling</a></li>
<li><a href="#data-extraction">Data extraction</a></li>
<li><a href="#data-processing">Data processing</a></li>
<li><a href="#creating-the-datasets">Creating the datasets</a>
<ul>
<li><a href="#spitting-the-images-into-train-validate-and-test">Spitting the images into train, validate, and test</a></li>
</ul></li>
</ul></li>
<li><a href="#compiling-the-model">Compiling the model</a>
<ul>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#optimizer">Optimizer</a>
<ul>
<li><a href="#adam-optimizer">Adam optimizer</a></li>
</ul></li>
<li><a href="#metrics">Metrics</a></li>
<li><a href="#defining-the-model">Defining the model</a></li>
</ul></li>
<li><a href="#training-the-baseline-model">Training the baseline model</a>
<ul>
<li><a href="#baseline-loss-and-accuracy-plots">Baseline loss and accuracy plots</a></li>
<li><a href="#baseline-evaluation">Baseline evaluation</a></li>
<li><a href="#baseline-confusion-matrix">Baseline confusion matrix</a></li>
</ul></li>
<li><a href="#training-the-optimized-model">Training the optimized model</a>
<ul>
<li><a href="#preventing-overfitting">Preventing overfitting</a>
<ul>
<li><a href="#data-augmentation">Data augmentation</a></li>
<li><a href="#learning-rate-decay">Learning rate decay</a></li>
</ul></li>
<li><a href="#callbacks">Callbacks</a></li>
<li><a href="#loss-and-accuracy-plots">Loss and accuracy plots</a></li>
<li><a href="#model-evaluation">Model evaluation</a></li>
<li><a href="#confusion-matrix">Confusion matrix</a></li>
<li><a href="#comparing-the-baseline-model-to-the-optimized-model">Comparing the baseline model to the optimized model</a></li>
</ul></li>
</ul></li>
</ul>

<p></details></p>

<hr />

<h2 id="classification">Classification</h2>

<h3 id="binary-classification">Binary classification</h3>

<h3 id="multiclass-classification">Multiclass classification</h3>

<h4 id="multiclass-multilabel-classification">Multiclass multilabel classification</h4>

<p>We could beef up the multiclass classification to a multilabel multiclass classification: Cog state/level/hp/name/suit.
But this adds unneeded complexity.
Let's keep it simple.
In the future, I will surely add the classification of the Cog's state: battle, patrolling, spawning, de-spawning, etc.</p>

<hr />

<h2 id="toontown">ToonTown</h2>

<h3 id="toon">Toon</h3>

<p>There are 11 unique animals:</p>

<ul>
<li>bear</li>
<li>cat</li>
<li>crocodile</li>
<li>deer</li>
<li>dog</li>
<li>duck</li>
<li>horse</li>
<li>monkey</li>
<li>mouse</li>
<li>pig</li>
<li>rabbit</li>
</ul>

<p>Each animal can have a unique head shape, body length, and height.
Furthermore, each animal can have mismatching colors for its head, arms, and legs.</p>

<p><details>
    <summary>All animal species</summary>
    This dataset lacks mismatched-colored Toons.
    As you can see below, all Toons have matching colors for their head, arms, and legs.
    <figure class="center">
        <img src="img/unique_animals.png" style="width:100%;background:white;"/>
        <figcaption>11 unique animal species in ToonTown</figcaption>
    </figure>
</details></p>

<h3 id="cog">Cog</h3>

<p>There are 4 Cog suits: Bossbot, Lawbot, Cashbot, and Sellbot.
Each suit has 8 Cogs, totaling 32 unique Cogs.</p>

<h3 id="why-is-it-important-for-toons-to-classify-cogs">Why is it important for Toons to classify Cogs?</h3>

<p>ToonTasks, object avoidance</p>

<hr />

<h2 id="the-toonvision-dataset">The ToonVision dataset</h2>

<p>There doesn't exist a dataset for ToonVision, so I'll be creating one from scratch.
The following sections will explain my process and results.</p>

<h3 id="dataset-considerations">Dataset considerations</h3>

<ul>
<li>Images are split into training, validation, and test sets: 40% training, 20% validation, and 40% test.</li>
<li>Images of Toons and Cogs must be...
<ul>
<li>Taken at various distances from each street, not playground</li>
<li>Taken of the entity's front, back, and side</li>
</ul></li>
<li>In the Cog dataset, there must be an equal part of each Cog suit
<ul>
<li>There must must be an equal part of each unique Cog (32 unique Cogs)
<ul>
<li>There is a minimum requirement of 20 images per unique Cog (32*20 = 640 images total)</li>
</ul></li>
<li>Must not include the Cog's nametag in the image</li>
</ul></li>
<li>There must be an equal part of Toons and Cogs in each set
<ul>
<li>There must be an equal part Cog suit in each set</li>
</ul></li>
<li>In the Toon dataset, balance of animal types is welcome but not necessary</li>
</ul>

<h3 id="filename-and-data-folder-structure">Filename and data folder structure</h3>

<p>Cog filename structure: <code>cog_&lt;suit&gt;_&lt;name&gt;_&lt;index&gt;.png</code>.</p>

<p>Toon filename structure: <code>toon_&lt;animal&gt;_&lt;index&gt;.png</code>.</p>

<p>Data folder structure:</p>

<pre><code>img
├───data
│   ├───test
│   │   ├───cog
│   │   └───toon
│   ├───train
│   │   ├───cog
│   │   └───toon
│   └───validate
│       ├───cog
│       └───toon
├───raw
│   ├───processed
│   └───screenshots
│       ├   sample_img0.png
│       ├   sample_img0.xml
│       ├   sample_img1.png
│       └   sample_img1.xml
└───unsorted
    ├───cog
    └───toon
</code></pre>

<p>There's no need for a unique folder for each Cog suit because we can filter on the filename.</p>

<h3 id="data-acquisition">Data acquisition</h3>

<p>Acquiring data is simple: Walk around TT streets, take screenshots, and save them to the raw folder.
It's important to take screenshots from various distance and of different angles of each entity: front, back, and side.
Taking screenshots from up close is preferred.
When taken from far away, the entity's nametag covers the entity's head, thus causing us to crop the entity's head or include the nametag - neither are good options.</p>

<figure class="center" style="width:auto;">
    <img src="img/sample_screenshot.png" style="width:100%;"/>
    <figcaption>Sample screenshot containing one Cog and three Toons in battle</figcaption>
</figure>

<p>There were a few difficulties with acquiring data:</p>

<ol>
<li>Entities are typically moving unless in battle</li>
<li>Entities often obstruct other entities, which makes for less than ideal training data</li>
<li>Finding the desired entity is purely a matter of walking around the street and looking for the entity, there's no precision radar</li>
</ol>

<p>Furthermore, there were class-specific data acquisition problems:</p>

<ol>
<li>Cogs are more commonly found in the streets than Toons</li>
<li>Multi-colored Toons are uncommon, therefore the dataset is skewed towards single-colored Toons</li>
<li>The two highest-tiered Cogs are only found in Cog buildings, not in the streets (unless there's an invasion)
<ul>
<li>Highest-tiered Cogs include Corporate Raiders and The Big Cheese for Bossbots, Legal Eagle and Big Wig for Lawbots, etc.</li>
</ul></li>
</ol>

<p>As a result, we have an imbalanced dataset.
I hope to balance the dataset over time, but we'll work with the current imbalanced dataset to make our lives harder.
After using this dataset, we'll have a better understanding of how to deal with a model overfitting to a small dataset.</p>

<figure class="center">
    <img src="img/dataset_balance.png" style="width:100%;background:white;"/>
    <figcaption></figcaption>
</figure>

<p>The green lines indicate the <font style="color:#0F0;">desired number of samples</font> for each class, whereas the red lines indicate the <font style="color:red;">average number of samples</font> per class.</p>

<p>The dataset shows a few overrepresented classes:</p>

<ul>
<li>More Cogs than Toons (526 vs 148 Toon samples)</li>
<li>Too many cats on the streets (or I have a bias towards taking photos of cats)</li>
<li>Not enough horses on the streets (or I have a bias towards not taking photos of horses)</li>
</ul>

<h4 id="can-we-use-gans-to-synthesize-additional-data">Can we use GANs to synthesize additional data?</h4>

<p>Yes, iff there was a GAN that could generate Toons and Cogs.
As far as I know, no GAN exists for generating ToonTown entities; perhaps I can take a swing at it later.</p>

<p>Don't be discouraged about an imbalanced dataset.
We can use many techniques to adjust for the imbalance and still create an accurate model.</p>

<h3 id="data-labeling">Data labeling</h3>

<p>I'm using <a href="https://github.com/tzutalin/labelImg">labelimg</a> to draw labeled bounding boxes around Toons and Cogs.
Labels - also referred to as <code>obj_name</code> - follow the format:</p>

<ul>
<li><code>cog_&lt;bb|lb|cb|sb&gt;_&lt;name&gt;_&lt;index&gt;</code></li>
<li><code>toon_&lt;animal&gt;_&lt;index&gt;</code></li>
</ul>

<p>The cog labels contain shorthand notation (<code>&lt;bb|lb|cb|sb&gt;</code>) for each suit: Bossbot, Lawbot, Cashbot, and Sellbot, respectively.
This shorthand notation allows us to filter cog data by filename and create a classifier that can distinguish between the 4 suits.</p>

<p>Bounding boxes are saved in XML format - specifically <a href="https://mlhive.com/2022/02/read-and-write-pascal-voc-xml-annotations-in-python">Pascal VOC XML</a> - alongside the image in the <code>raw/screenshots</code> directory, as seen in the <a href="#filename-and-data-folder-structure">data folder file structure</a> section above.</p>

<figure class="center" style="width:auto;">
    <img src="img/sample_screenshot_with_bounding_boxes.png" style="width:100%;"/>
    <figcaption>Sample screenshot with labeled bounding boxes</figcaption>
</figure>

<p>How the objects are labeled - how the bounding boxes are drawn - determines how the object will be extracted from the image.
It's important to draw bounding boxes such that the entity is snugly contained within the bounding box.
Furthermore, we must exclude entity nametags in the bounding box because the classifier will learn to "cheat" by identifying objects from their nametag rather than features of the entity itself.</p>

<h3 id="data-extraction">Data extraction</h3>

<p>The raw data (screenshot) is passed into the <code>data_processing.py</code> script.
The script utilizes functions in <code>img_utils.py</code> to extract objects from the images using the labeled bounding boxes found in the image's corresponding XML files.
Specifically, the data extraction workflow is as follows:</p>

<ul>
<li>Acquire bounding box dimensions and labels from the XML files</li>
<li>Extract object (Toon or Cog) from the image using the dimensions and labels found in the XML files</li>
<li>Save the cropped image of the object to the <code>img/unsorted</code> folder</li>
<li>Move the raw image and its corresponding XML file to the <code>raw/processed</code> folder</li>
</ul>

<p><em>Why move the cropped image to unsorted and then processed?</em></p>

<p>The unsorted images directory is used to maintain a counter (referred to as an index) for each label.
It gives me a glimpse of how many images are in each category by looking at the filenames in the <code>unsorted</code> directory.
If I want to add more images to the dataset, I would have place images from all datasets back into the <code>unsorted</code> directory in order to maintain the counter and avoid overwriting existing images.</p>

<p>Given that my dataset is so small, I can unsort and re-sort the images with ease.
But this is not at all scalable in the future and I will surely redesign this portion of the data pipeline.</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># %% Convert raw images to data images</span>
<span class="k">def</span> <span class="nf">process_images</span><span class="p">(</span>
    <span class="n">raw_images_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">SCREENSHOTS_DIR</span><span class="p">,</span>
    <span class="n">image_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;png&quot;</span><span class="p">,</span>
    <span class="n">move_images</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Extract objects from raw images and save them to the unsorted img directory&quot;&quot;&quot;</span>
    <span class="n">screenshots</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">raw_images_dir</span><span class="si">}</span><span class="s2">/*.</span><span class="si">{</span><span class="n">image_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">screenshots</span><span class="p">)</span><span class="si">}</span><span class="s2"> screenshots&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="n">screenshots</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing </span><span class="si">{</span><span class="n">img_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">xml_path</span> <span class="o">=</span> <span class="n">img_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;.</span><span class="si">{</span><span class="n">image_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;.xml&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">xml_path</span><span class="p">):</span>
            <span class="c1"># Extract objects&#39; labels and bounding box dimensions from XML</span>
            <span class="n">objs_from_xml</span> <span class="o">=</span> <span class="n">extract_objects_from_xml</span><span class="p">(</span><span class="n">xml_path</span><span class="p">)</span>
            <span class="c1"># Extract objects from images using XML data</span>
            <span class="n">objs_from_img</span> <span class="o">=</span> <span class="n">extract_objects_from_img</span><span class="p">(</span><span class="n">img_path</span><span class="p">,</span> <span class="n">objs_from_xml</span><span class="p">)</span>
            <span class="c1"># Save extracted objects to images, modify image name to include object index</span>
            <span class="n">save_objects_to_img</span><span class="p">(</span><span class="n">objs_from_img</span><span class="p">,</span> <span class="n">UNSORTED_DIR</span><span class="p">)</span>
            <span class="c1"># Move raw image to processed directory</span>
            <span class="k">if</span> <span class="n">move_images</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="p">[</span><span class="n">img_path</span><span class="p">,</span> <span class="n">xml_path</span><span class="p">]:</span>
                    <span class="n">new_path</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">raw_images_dir</span><span class="p">,</span> <span class="n">PROCESSED_DIR</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Moving </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">new_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">rename</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">new_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    No XML file found for </span><span class="si">{</span><span class="n">img_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="data-processing">Data processing</h3>

<figure class="right">
    <img src="img/image_size_scale.png" style="width:100%;"/>
    <figcaption></figcaption>
</figure>

<p>The extracted objects are of various sizes because the screenshots were taken from various angles and distances.
Large objects are a result of the screenshot taken from up close, while small objects are a result of the screenshot taken from far away.</p>

<p>We can see from the image on the right how the object's distance affects the extracted object's quality.
The further the object is from the camera, the smaller and more blurry the object is.
We lose quite a bit of information about the object when the object is far away.</p>

<p>Overall, it would be ideal for the dataset to consist mostly of large, close-up objects because they contain more information about the object.
Small, far-away objects lose information about the object and are not as useful for training.
Further more, we could simulate this loss of information through image augmentation - rescaling or blurring/pixelating the image.</p>

<p>It would make a fun project to create a model that upscales the images to a higher resolution.
Then we could use those high-resolution images in this dataset.</p>

<h3 id="creating-the-datasets">Creating the datasets</h3>

<p>After the objects are extracted and placed in the <code>unsorted</code> folder, we can create the datasets.
First, we need to create a balanced datasets within the <code>data/[train|validate|test]</code> folders.
Remember that we're aiming for a 60/20/20 split of the dataset for training, validation, and testing, respectively.</p>

<h4 id="spitting-the-images-into-train-validate-and-test">Spitting the images into train, validate, and test</h4>

<p>Before creating the datasets, we need to move images from <code>unsorted/[cog|toon]</code> to <code>data/[train|validate|test]/[cog|toon]</code>.</p>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">split_data</span><span class="p">(</span><span class="n">dry_run</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Split the data into train(40%)/validate(20%)/test(40%) data sets&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">unsorted_dir</span> <span class="ow">in</span> <span class="p">[</span><span class="n">UNSORTED_COG_DIR</span><span class="p">,</span> <span class="n">UNSORTED_TOON_DIR</span><span class="p">]:</span>
        <span class="n">cog_or_toon</span> <span class="o">=</span> <span class="n">unsorted_dir</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Get all images from unsorted_dir</span>
        <span class="n">unsorted_images</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">unsorted_dir</span><span class="si">}</span><span class="s2">/*.png&quot;</span><span class="p">)</span>
        <span class="n">num_images</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">unsorted_images</span><span class="p">)</span>

        <span class="c1"># Split images into train/validate/test sets</span>
        <span class="n">num_train</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_images</span> <span class="o">*</span> <span class="mf">0.4</span><span class="p">)</span>
        <span class="n">num_validate</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_images</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
        <span class="n">num_test</span> <span class="o">=</span> <span class="n">num_images</span> <span class="o">-</span> <span class="n">num_train</span> <span class="o">-</span> <span class="n">num_validate</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">num_train</span><span class="p">,</span> <span class="n">num_validate</span><span class="p">,</span> <span class="n">num_test</span><span class="p">)</span>

        <span class="c1"># # Shuffle filenames to randomize the order of the images</span>
        <span class="n">shuffle</span><span class="p">(</span><span class="n">unsorted_images</span><span class="p">)</span>
        <span class="n">train</span> <span class="o">=</span> <span class="n">unsorted_images</span><span class="p">[:</span><span class="n">num_train</span><span class="p">]</span>
        <span class="n">validate</span> <span class="o">=</span> <span class="n">unsorted_images</span><span class="p">[</span><span class="n">num_train</span><span class="p">:</span><span class="o">-</span><span class="n">num_test</span><span class="p">]</span>
        <span class="n">test</span> <span class="o">=</span> <span class="n">unsorted_images</span><span class="p">[</span><span class="o">-</span><span class="n">num_test</span><span class="p">:]</span>

        <span class="c1"># Move images to train/validate/test directories</span>
        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">dir_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">train</span><span class="p">,</span> <span class="n">validate</span><span class="p">,</span> <span class="n">test</span><span class="p">],</span> <span class="p">[</span><span class="n">TRAIN_DIR</span><span class="p">,</span> <span class="n">VALIDATE_DIR</span><span class="p">,</span> <span class="n">TEST_DIR</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
                <span class="n">new_path</span> <span class="o">=</span> <span class="n">img_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">unsorted_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dir_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">cog_or_toon</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">dry_run</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Moving </span><span class="si">{</span><span class="n">img_path</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">new_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">rename</span><span class="p">(</span><span class="n">img_path</span><span class="p">,</span> <span class="n">new_path</span><span class="p">)</span>
</code></pre></div>

<p>We can visualize the balances of the datasets using the <code>plot_all_datasets</code> function in <code>data_visualization.py</code>.</p>

<p><font style="color:red">TODO: Insert image of dataset balance</font></p>

<p>The creation of datasets is straight-forward using keras:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># %% Create datasets</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.image_dataset_from_directory</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">image_dataset_from_directory</span><span class="p">(</span>
    <span class="n">TRAIN_DIR</span><span class="p">,</span>
    <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="c1"># batch_size=16</span>
<span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">image_dataset_from_directory</span><span class="p">(</span>
    <span class="n">VALIDATE_DIR</span><span class="p">,</span>
    <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="c1"># batch_size=16</span>
<span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">image_dataset_from_directory</span><span class="p">(</span>
    <span class="n">TEST_DIR</span><span class="p">,</span>
    <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="c1"># batch_size=16</span>
<span class="p">)</span>
</code></pre></div>

<p><font style="color:red">TODO: Insert sample images from train dataset</font></p>

<h2 id="compiling-the-model">Compiling the model</h2>

<p>Now that we've created the datasets, we can compile the model.
Compiling the model requires choosing a loss function, optimizer, and metrics to monitor the model's performance during training.</p>

<h3 id="loss-function">Loss function</h3>

<p>Our first model is classifying between two classes, therefore we'll use the <code>binary_crossentropy</code> loss function.
The later models will be more complex - classifying 4 or 32 classes - so we'll use the <code>categorical_crossentropy</code> loss function.</p>

<h3 id="optimizer">Optimizer</h3>

<p>There are a few guidelines to choosing an optimizer for classification problems.
I visualized the process by plotting optimizers' performances using the same model, dataset, hyperparameters, and number of epochs.</p>

<p><font style="color:red">TODO: Insert plot</font></p>

<p>A handful of the optimizers' losses flattened over the course of training as a result of a low learning rate or <em>vanishing gradients</em>.
SGD commonly encounters this problem, and it's often due to a low learning rate.</p>

<p>I increased the learning rate for all flattened optimizers and plotted the loss scores and accuracies again.</p>

<p><font style="color:red">TODO: Insert plot with non-equal learning rates</font></p>

<p>I could use additional callbacks, such as the learning rate scheduler, to gradually decrease the learning rate and improve the model's performance
Even more, adding <em>momentum</em> to the SGD optimizer could help the model reach global loss minimums and learn more effectively.
Alternatively, I could read an article which discusses the points above and use their suggestions to save time, but where's the fun in that?</p>

<p>Given the loss scores and accuracies plotted above, I've chosen to go with the <code>Adam</code> optimizer.</p>

<h4 id="adam-optimizer">Adam optimizer</h4>

<p>The Adam optimizer is a variant of the stochastic gradient descent (SGD) algorithm.
Adam combines the advantages of two other SGD variants - "AdaGrad" and "RMSProp" - to create a more effective optimization algorithm for computer vision tasks.
Jason Brownlee wrote an <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">excellent introduction</a> to the Adam algorithm, and I encourage you to read it if you're interested in the technical details.</p>

<h3 id="metrics">Metrics</h3>

<p>It's a classification model with only two classes, so we'll use the <code>binary_accuracy</code> metric.
We'll use the <code>categorical_accuracy</code> metric later for our model with 4 and 32 classes.
That's about it.</p>

<h3 id="defining-the-model">Defining the model</h3>

<h2 id="training-the-baseline-model">Training the baseline model</h2>

<p>Before training the actual model, we need to define a simple baseline to compare against.
The baseline model will use the same model architecture, datasets, and hyperparameters as the optimized model we're training.
The only difference is that we will not perform any optimizations - no data augmentation, dropout, batch normalization or learning rate decay.</p>

<p>We'll train the baseline for 25 epochs with a learning rate of 0.001 (1e-3).
The baseline model will be trained 50 times, each time with a rebalanced dataset.
The average of all 50 runs will be plotted below.</p>

<div class="codehilite"><pre><span></span><code><span class="n">model_baseline</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="c1"># ! TODO`</span>
</code></pre></div>

<h3 id="baseline-loss-and-accuracy-plots">Baseline loss and accuracy plots</h3>

<h3 id="baseline-evaluation">Baseline evaluation</h3>

<h3 id="baseline-confusion-matrix">Baseline confusion matrix</h3>

<h2 id="training-the-optimized-model">Training the optimized model</h2>

<h3 id="preventing-overfitting">Preventing overfitting</h3>

<p>Given that we have a small dataset, we can utilize a few of the following techniques during training to prevent overfitting:</p>

<ul>
<li><strong>Data augmentation</strong> - we can augment the images by randomly rotating, flipping, and cropping them.</li>
<li><strong>Data balancing</strong> - we can balance the datasets by balancing the number of objects in each dataset.</li>
<li><strong>Dropout</strong> - we can dropout some of the nodes in the model to prevent overfitting.</li>
<li><strong>Regularization</strong> - we can regularize the model by adding a penalty to the loss function.</li>
<li><strong>Small learning rate</strong> - we can use a small learning rate to prevent overfitting.</li>
<li><strong>Reducing number of parameters</strong> - too many parameters can cause overfitting.</li>
<li><strong>Early stopping</strong> - we can stop training the model if the model doesn't improve after a certain number of epochs.</li>
</ul>

<p>For the ToonVision model, we'll utilize data augmentation, dropout, regularization, small learning rate, and learning rate decay.</p>

<h4 id="data-augmentation">Data augmentation</h4>

<p>During training, we'll perform the following data augmentation techniques:</p>

<ul>
<li>Horizontal flip (50% chance)</li>
<li>Rotate +- 20%</li>
<li>Zoom +-30%</li>
</ul>

<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">data_augmentation</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># Apply horizontal flipping to 50% of the images</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal&quot;</span><span class="p">),</span>
        <span class="c1"># Rotate the input image by some factor in range [-20%, 20%] or [-72, 72] in degrees</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="c1"># Zoom in or out by a random factor in range [-30%, 30%]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">RandomZoom</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="p">])</span>
</code></pre></div>

<h4 id="learning-rate-decay">Learning rate decay</h4>

<p>Learning rate decay is a technique that helps prevent overfitting.
It's a simple way to reduce the learning rate of the optimizer as training progresses.
We can implement learning rate decay in two ways:</p>

<ul>
<li>Use a learning rate scheduler (callback)</li>
<li>Add a learning rate decay factor to the optimizer</li>
</ul>

<p>I'll use the optimizer's <code>lr_decay</code> argument to implement the learning rate decay in training.</p>

<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">LearningRateScheduler</span>

<span class="k">def</span> <span class="nf">lr_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.1</span> <span class="o">**</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">//</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Create a learning rate scheduler</span>
<span class="n">lr_callback</span> <span class="o">=</span> <span class="n">LearningRateScheduler</span><span class="p">(</span><span class="n">lr_schedule</span><span class="p">)</span>
<span class="c1"># Add the learning rate decay to the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">lr_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div>

<h3 id="callbacks">Callbacks</h3>

<h3 id="loss-and-accuracy-plots">Loss and accuracy plots</h3>

<h3 id="model-evaluation">Model evaluation</h3>

<h3 id="confusion-matrix">Confusion matrix</h3>

<h3 id="comparing-the-baseline-model-to-the-optimized-model">Comparing the baseline model to the optimized model</h3>

<!-- Split the training line chart and the evaluation bar chart -->

<figure class="center" style="width:100%;">
    <img src="img/baseline_comparison_avg50runs_25epochs.png" style="width:100%;"/>
    <figcaption></figcaption>
</figure>

<!-- Split the training line chart and the evaluation bar chart -->

<figure class="center" style="width:100%;">
    <img src="img/baseline_comparison_train.png" style="width:100%;"/>
    <figcaption></figcaption>
</figure>

<!-- Split the training line chart and the evaluation bar chart -->

<figure class="center" style="width:100%;">
    <img src="img/baseline_comparison_test.png" style="width:100%;"/>
    <figcaption></figcaption>
</figure>

</body>
</html>
